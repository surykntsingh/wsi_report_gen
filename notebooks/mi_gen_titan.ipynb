{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998e4abf-f34c-431e-82a7-bfa175f06ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, PackedSequence, pack_padded_sequence, pad_packed_sequence\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.text.bleu import BLEUScore\n",
    "\n",
    "from copy import deepcopy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a848a4-e40b-4d42-882a-811526c7dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH = '/data04/shared/skapse/REG_processed/20x_512px_0px_overlap'\n",
    "REPORTS_JSON_PATH = '/home/sbsingh/projects/report_gen/train.json'\n",
    "model_ckpt_path = '/home/sbsingh/projects/report_gen/pretrained/model_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f67489-27fe-4a1a-9d20-05379e55cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(*message):\n",
    "    if debug:\n",
    "        print('_______'.join(message))\n",
    "\n",
    "def read_json_file(json_path):\n",
    "        with open(json_path) as f:\n",
    "            d = json.load(f)\n",
    "        return d\n",
    "\n",
    "def penalty_builder(penalty_config):\n",
    "    if penalty_config == '':\n",
    "        return lambda x, y: y\n",
    "    pen_type, alpha = penalty_config.split('_')\n",
    "    alpha = float(alpha)\n",
    "    if pen_type == 'wu':\n",
    "        return lambda x, y: length_wu(x, y, alpha)\n",
    "    if pen_type == 'avg':\n",
    "        return lambda x, y: length_average(x, y, alpha)\n",
    "\n",
    "\n",
    "def length_wu(length, logprobs, alpha=0.):\n",
    "    \"\"\"\n",
    "    NMT length re-ranking score from\n",
    "    \"Google's Neural Machine Translation System\" :cite:`wu2016google`.\n",
    "    \"\"\"\n",
    "\n",
    "    modifier = (((5 + length) ** alpha) /\n",
    "                ((5 + 1) ** alpha))\n",
    "    return logprobs / modifier\n",
    "\n",
    "\n",
    "def length_average(length, logprobs, alpha=0.):\n",
    "    \"\"\"\n",
    "    Returns the average probability of tokens in a sequence.\n",
    "    \"\"\"\n",
    "    if length<alpha:\n",
    "        penalty= -1000\n",
    "    else:\n",
    "        penalty = logprobs / length\n",
    "    log_message(f'length: {length}, logprobs: {logprobs}, penalty: {penalty}')\n",
    "    return penalty\n",
    "\n",
    "\n",
    "def split_tensors(n, x):\n",
    "    if torch.is_tensor(x):\n",
    "        assert x.shape[0] % n == 0\n",
    "        x = x.reshape(x.shape[0] // n, n, *x.shape[1:]).unbind(1)\n",
    "    elif type(x) is list or type(x) is tuple:\n",
    "        x = [split_tensors(n, _) for _ in x]\n",
    "    elif x is None:\n",
    "        x = [None] * n\n",
    "    return x\n",
    "\n",
    "\n",
    "def repeat_tensors(n, x):\n",
    "    \"\"\"\n",
    "    For a tensor of size Bx..., we repeat it n times, and make it Bnx...\n",
    "    For collections, do nested repeat\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.unsqueeze(1)  # Bx1x...\n",
    "        x = x.expand(-1, n, *([-1] * len(x.shape[2:])))  # Bxnx...\n",
    "        x = x.reshape(x.shape[0] * n, *x.shape[2:])  # Bnx...\n",
    "    elif type(x) is list or type(x) is tuple:\n",
    "        x = [repeat_tensors(n, _) for _ in x]\n",
    "    return x\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def pad_tokens(att_feats):\n",
    "    # ---->pad\n",
    "    H = att_feats.shape[1]\n",
    "    _H, _W = int(np.ceil(np.sqrt(H))), int(np.ceil(np.sqrt(H)))\n",
    "    add_length = _H * _W - H\n",
    "    att_feats = torch.cat([att_feats, att_feats[:, :add_length, :]], dim=1)  # [B, N, L]\n",
    "    return att_feats\n",
    "\n",
    "def sort_pack_padded_sequence(input, lengths):\n",
    "    lengths =lengths.cpu()\n",
    "    # log_message(f'lengths: {lengths}')\n",
    "    sorted_lengths, indices = torch.sort(lengths, descending=True)\n",
    "    # log_message(input[indices], sorted_lengths)\n",
    "    tmp = pack_padded_sequence(input[indices], sorted_lengths, batch_first=True)\n",
    "    inv_ix = indices.clone()\n",
    "    inv_ix[indices] = torch.arange(0, len(indices)).type_as(inv_ix)\n",
    "    return tmp, inv_ix\n",
    "\n",
    "\n",
    "def pad_unsort_packed_sequence(input, inv_ix):\n",
    "    tmp, _ = pad_packed_sequence(input, batch_first=True)\n",
    "    tmp = tmp[inv_ix]\n",
    "    return tmp\n",
    "\n",
    "def pack_wrapper(module, att_feats, att_masks):\n",
    "    # print(module, att_feats, att_masks)\n",
    "    if att_masks is not None:\n",
    "        packed, inv_ix = sort_pack_padded_sequence(att_feats, att_masks.data.long().sum(1))\n",
    "        return pad_unsort_packed_sequence(PackedSequence(module(packed[0]), packed[1]), inv_ix)\n",
    "    else:\n",
    "        return module(att_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1972a8f-9199-4cfd-bfd0-799c845a77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, reports_json_path, threshold = 1):\n",
    "        self.__threshold = threshold\n",
    "        self.__token2idx, self.__idx2token = self.create_vocabulary(reports_json_path)\n",
    "\n",
    "\n",
    "    def create_vocabulary(self, reports_json_path):\n",
    "        total_tokens = []\n",
    "        reports = read_json_file(reports_json_path)\n",
    "\n",
    "        for report in reports:\n",
    "            tokens = report['report'].split()\n",
    "            # for token in tokens:\n",
    "            #     total_tokens.append(token)\n",
    "            total_tokens.extend(tokens)\n",
    "\n",
    "        counter = Counter(total_tokens)\n",
    "        vocab = [k for k, v in counter.items() if v >= self.__threshold] + ['<unk>']\n",
    "        vocab.sort()\n",
    "        token2idx, idx2token = {}, {}\n",
    "        for idx, token in enumerate(vocab):\n",
    "            token2idx[token] = idx + 1\n",
    "            idx2token[idx + 1] = token\n",
    "\n",
    "        return token2idx, idx2token\n",
    "\n",
    "    def get_token_by_id(self, id):\n",
    "        return self.__idx2token[id]\n",
    "\n",
    "    def get_id_by_token(self, token):\n",
    "        if token not in self.__token2idx:\n",
    "            return self.__token2idx['<unk>']\n",
    "        return self.__token2idx[token]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.__token2idx)\n",
    "\n",
    "    def __call__(self, report):\n",
    "        tokens = report.split()\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.get_id_by_token(token))\n",
    "        ids = [0] + ids + [0]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # print(ids)\n",
    "        txt = ''\n",
    "        for i, idx in enumerate(ids):\n",
    "            if idx > 0:\n",
    "                if i >= 1:\n",
    "                    txt += ' '\n",
    "                txt += self.get_token_by_id(idx)\n",
    "            else:\n",
    "                break\n",
    "        return txt\n",
    "\n",
    "    def batch_decode(self, ids_batch):\n",
    "        # print(f'ids_batch: {ids_batch}, {ids_batch.shape}')\n",
    "        out = []\n",
    "        for ids in ids_batch:\n",
    "            out.append(self.decode(ids))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568c4b94-4a40-42c8-9aa0-1d70134f2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, embeddings_path, reports_json_path, tokenizer, max_seq_length):\n",
    "        reports = read_json_file(reports_json_path)\n",
    "        self.__reports = {report['id'].split('.')[0]: report['report'] for report in reports}\n",
    "        self.__tokenizer = tokenizer\n",
    "        self.__embeddings_path = embeddings_path\n",
    "        self.__max_seq_length = max_seq_length\n",
    "\n",
    "        files = os.listdir(embeddings_path)\n",
    "        self.__slides = [file.split('.')[0] for file in files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__slides)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id = self.__slides[idx]\n",
    "        with h5py.File(f'{self.__embeddings_path}/{slide_id}.h5', \"r\") as h5_file:\n",
    "            coords_np = h5_file[\"coords\"][:]\n",
    "            embeddings_np = h5_file[\"features\"][:]\n",
    "\n",
    "            coords = torch.tensor(coords_np).float() \n",
    "            embedding = torch.tensor(embeddings_np)\n",
    "            report_text = self.__reports[slide_id]\n",
    "            report_ids = self.__tokenizer(report_text)\n",
    "\n",
    "            if len(report_ids) < self.__max_seq_length:\n",
    "                padding = [0] * (self.__max_seq_length-len(report_ids))\n",
    "                report_ids.extend(padding)\n",
    "\n",
    "            report_masks = [1] * len(report_ids)\n",
    "            seq_length = len(report_ids)\n",
    "\n",
    "\n",
    "        return slide_id, embedding.unsqueeze(0), coords, report_ids, report_masks, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfcc13fe-77ae-4431-8c23-5d8d3cfa196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(REPORTS_JSON_PATH)\n",
    "embeddings_path = f'{EMB_PATH}/slide_features_titan'\n",
    "max_seq_length = 300\n",
    "embedding_dataset = EmbeddingDataset(embeddings_path, REPORTS_JSON_PATH, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36ba08f-6ae4-479a-95e6-6e4cfb205686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af189cce-2505-4f09-bf13-1eac1dd9e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # super(CaptionModel, self).__init__()\n",
    "\n",
    "    # implements beam search\n",
    "    # calls beam_step and returns the final set of beams\n",
    "    # augments log-probabilities with diversity terms when number of groups > 1\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        mode = kwargs.get('mode', 'forward')\n",
    "        if 'mode' in kwargs:\n",
    "            del kwargs['mode']\n",
    "        return getattr(self, '_' + mode)(*args, **kwargs)\n",
    "\n",
    "    def beam_search(self, init_state, init_logprobs, *args, **kwargs):\n",
    "\n",
    "        # function computes the similarity score to be augmented\n",
    "        def add_diversity(beam_seq_table, logprobs, t, divm, diversity_lambda, bdash):\n",
    "            local_time = t - divm\n",
    "            unaug_logprobs = logprobs.clone()\n",
    "            batch_size = beam_seq_table[0].shape[0]\n",
    "\n",
    "            if divm > 0:\n",
    "                change = logprobs.new_zeros(batch_size, logprobs.shape[-1])\n",
    "                for prev_choice in range(divm):\n",
    "                    prev_decisions = beam_seq_table[prev_choice][:, :, local_time]  # Nxb\n",
    "                    for prev_labels in range(bdash):\n",
    "                        change.scatter_add_(1, prev_decisions[:, prev_labels].unsqueeze(-1),\n",
    "                                            change.new_ones(batch_size, 1))\n",
    "\n",
    "                if local_time == 0:\n",
    "                    logprobs = logprobs - change * diversity_lambda\n",
    "                else:\n",
    "                    logprobs = logprobs - repeat_tensors(bdash, change) * diversity_lambda\n",
    "\n",
    "            return logprobs, unaug_logprobs\n",
    "\n",
    "        # does one step of classical beam search\n",
    "\n",
    "        def beam_step(logprobs, unaug_logprobs, beam_size, t, beam_seq, beam_seq_logprobs, beam_logprobs_sum, state):\n",
    "            # INPUTS:\n",
    "            # logprobs: probabilities augmented after diversity N*bxV\n",
    "            # beam_size: obvious\n",
    "            # t        : time instant\n",
    "            # beam_seq : tensor contanining the beams\n",
    "            # beam_seq_logprobs: tensor contanining the beam logprobs\n",
    "            # beam_logprobs_sum: tensor contanining joint logprobs\n",
    "            # OUPUTS:\n",
    "            # beam_seq : tensor containing the word indices of the decoded captions Nxbxl\n",
    "            # beam_seq_logprobs : log-probability of each decision made, NxbxlxV\n",
    "            # beam_logprobs_sum : joint log-probability of each beam Nxb\n",
    "\n",
    "            batch_size = beam_logprobs_sum.shape[0]\n",
    "            vocab_size = logprobs.shape[-1]\n",
    "            logprobs = logprobs.reshape(batch_size, -1, vocab_size)  # NxbxV\n",
    "            if t == 0:\n",
    "                assert logprobs.shape[1] == 1\n",
    "                beam_logprobs_sum = beam_logprobs_sum[:, :1]\n",
    "            candidate_logprobs = beam_logprobs_sum.unsqueeze(-1) + logprobs  # beam_logprobs_sum Nxb logprobs is NxbxV\n",
    "            ys, ix = torch.sort(candidate_logprobs.reshape(candidate_logprobs.shape[0], -1), -1, True)\n",
    "            ys, ix = ys[:, :beam_size], ix[:, :beam_size]\n",
    "            beam_ix = ix // vocab_size  # Nxb which beam\n",
    "            selected_ix = ix % vocab_size  # Nxb # which world\n",
    "            state_ix = (beam_ix + torch.arange(batch_size).type_as(beam_ix).unsqueeze(-1) * logprobs.shape[1]).reshape(\n",
    "                -1)  # N*b which in Nxb beams\n",
    "\n",
    "            if t > 0:\n",
    "                # gather according to beam_ix\n",
    "                assert (beam_seq.gather(1, beam_ix.unsqueeze(-1).expand_as(beam_seq)) ==\n",
    "                        beam_seq.reshape(-1, beam_seq.shape[-1])[state_ix].view_as(beam_seq)).all()\n",
    "                beam_seq = beam_seq.gather(1, beam_ix.unsqueeze(-1).expand_as(beam_seq))\n",
    "\n",
    "                beam_seq_logprobs = beam_seq_logprobs.gather(1, beam_ix.unsqueeze(-1).unsqueeze(-1).expand_as(\n",
    "                    beam_seq_logprobs))\n",
    "\n",
    "            beam_seq = torch.cat([beam_seq, selected_ix.unsqueeze(-1)], -1)  # beam_seq Nxbxl\n",
    "            beam_logprobs_sum = beam_logprobs_sum.gather(1, beam_ix) + \\\n",
    "                                logprobs.reshape(batch_size, -1).gather(1, ix)\n",
    "            assert (beam_logprobs_sum == ys).all()\n",
    "            _tmp_beam_logprobs = unaug_logprobs[state_ix].reshape(batch_size, -1, vocab_size)\n",
    "            beam_logprobs = unaug_logprobs.reshape(batch_size, -1, vocab_size).gather(1,\n",
    "                                                                                      beam_ix.unsqueeze(-1).expand(-1,\n",
    "                                                                                                                   -1,\n",
    "                                                                                                                   vocab_size))  # NxbxV\n",
    "            assert (_tmp_beam_logprobs == beam_logprobs).all()\n",
    "            beam_seq_logprobs = torch.cat([\n",
    "                beam_seq_logprobs,\n",
    "                beam_logprobs.reshape(batch_size, -1, 1, vocab_size)], 2)\n",
    "\n",
    "            new_state = [None for _ in state]\n",
    "            for _ix in range(len(new_state)):\n",
    "                #  copy over state in previous beam q to new beam at vix\n",
    "                new_state[_ix] = state[_ix][:, state_ix]\n",
    "            state = new_state\n",
    "            return beam_seq, beam_seq_logprobs, beam_logprobs_sum, state\n",
    "\n",
    "        # Start diverse_beam_search\n",
    "        opt = kwargs['opt']\n",
    "        temperature = opt.get('temperature', 1)  # This should not affect beam search, but will affect dbs\n",
    "        beam_size = opt.get('beam_size', 10)\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        diversity_lambda = opt.get('diversity_lambda', 0.5)\n",
    "        decoding_constraint = opt.get('decoding_constraint', 0)\n",
    "        suppress_UNK = opt.get('suppress_UNK', 0)\n",
    "        length_penalty = penalty_builder(opt.get('length_penalty', ''))\n",
    "        bdash = beam_size // group_size  # beam per group\n",
    "\n",
    "        batch_size = init_logprobs.shape[0]\n",
    "        device = init_logprobs.device\n",
    "        # INITIALIZATIONS\n",
    "        beam_seq_table = [torch.LongTensor(batch_size, bdash, 0).to(device) for _ in range(group_size)]\n",
    "        beam_seq_logprobs_table = [torch.FloatTensor(batch_size, bdash, 0, self.vocab_size + 1).to(device) for _ in\n",
    "                                   range(group_size)]\n",
    "        beam_logprobs_sum_table = [torch.zeros(batch_size, bdash).to(device) for _ in range(group_size)]\n",
    "\n",
    "        # logprobs # logprobs predicted in last time step, shape (beam_size, vocab_size+1)\n",
    "        done_beams_table = [[[] for __ in range(group_size)] for _ in range(batch_size)]\n",
    "        state_table = [[_.clone() for _ in init_state] for _ in range(group_size)]\n",
    "        logprobs_table = [init_logprobs.clone() for _ in range(group_size)]\n",
    "        # END INIT\n",
    "\n",
    "        # Chunk elements in the args\n",
    "        args = list(args)\n",
    "        args = split_tensors(group_size, args)  # For each arg, turn (Bbg)x... to (Bb)x(g)x...\n",
    "        if self.__class__.__name__ == 'AttEnsemble':\n",
    "            args = [[[args[j][i][k] for i in range(len(self.models))] for j in range(len(args))] for k in\n",
    "                    range(group_size)]  # group_name, arg_name, model_name\n",
    "        else:\n",
    "            args = [[args[i][j] for i in range(len(args))] for j in range(group_size)]\n",
    "\n",
    "        for t in range(self.max_seq_length + group_size - 1):\n",
    "            for divm in range(group_size):\n",
    "                if t >= divm and t <= self.max_seq_length + divm - 1:\n",
    "                    # add diversity\n",
    "                    logprobs = logprobs_table[divm]\n",
    "                    # suppress previous word\n",
    "                    if decoding_constraint and t - divm > 0:\n",
    "                        logprobs.scatter_(1, beam_seq_table[divm][:, :, t - divm - 1].reshape(-1, 1).to(device),\n",
    "                                          float('-inf'))\n",
    "                    # suppress UNK tokens in the decoding\n",
    "                    if suppress_UNK:\n",
    "                        idx_unk = self.tokenizer.get_id_by_token('<unk>')\n",
    "                        logprobs[:, idx_unk] = logprobs[:, idx_unk] - 1000\n",
    "                        # diversity is added here\n",
    "                    # the function directly modifies the logprobs values and hence, we need to return\n",
    "                    # the unaugmented ones for sorting the candidates in the end. # for historical\n",
    "                    # reasons :-)\n",
    "                    logprobs, unaug_logprobs = add_diversity(beam_seq_table, logprobs, t, divm, diversity_lambda, bdash)\n",
    "\n",
    "                    # infer new beams\n",
    "                    beam_seq_table[divm], \\\n",
    "                    beam_seq_logprobs_table[divm], \\\n",
    "                    beam_logprobs_sum_table[divm], \\\n",
    "                    state_table[divm] = beam_step(logprobs,\n",
    "                                                  unaug_logprobs,\n",
    "                                                  bdash,\n",
    "                                                  t - divm,\n",
    "                                                  beam_seq_table[divm],\n",
    "                                                  beam_seq_logprobs_table[divm],\n",
    "                                                  beam_logprobs_sum_table[divm],\n",
    "                                                  state_table[divm])\n",
    "\n",
    "                    # log_message(f' t: {t}, divm: {divm}, beam_seq_table[divm]: {beam_seq_table[divm]}, beam_logprobs_sum_table: {beam_logprobs_sum_table}')\n",
    "                    # if time's up... or if end token is reached then copy beams\n",
    "                    for b in range(batch_size):\n",
    "                        is_end = beam_seq_table[divm][b, :, t - divm] == self.eos_idx\n",
    "                        assert beam_seq_table[divm].shape[-1] == t - divm + 1\n",
    "                        if t == self.max_seq_length + divm - 1:\n",
    "                            is_end.fill_(1)\n",
    "                        for vix in range(bdash):\n",
    "                            if is_end[vix]:\n",
    "                                final_beam = {\n",
    "                                    'seq': beam_seq_table[divm][b, vix].clone(),\n",
    "                                    'logps': beam_seq_logprobs_table[divm][b, vix].clone(),\n",
    "                                    'unaug_p': beam_seq_logprobs_table[divm][b, vix].sum().item(),\n",
    "                                    'p': beam_logprobs_sum_table[divm][b, vix].item()\n",
    "                                }\n",
    "                                log_message(f\"final_beam : {final_beam['seq']}, {final_beam['p']}, penalty: {length_penalty(t - divm + 1, final_beam['p'])}\")\n",
    "                                final_beam['p'] = length_penalty(t - divm + 1, final_beam['p'])\n",
    "                                done_beams_table[b][divm].append(final_beam)\n",
    "                        beam_logprobs_sum_table[divm][b, is_end] -= 1000\n",
    "                        \n",
    "                    # log_message(f\"done_beams_table: {list(map(lambda x: (x['seq'], x['p']), done_beams_table[0][0]))}\")\n",
    "                    # move the current group one step forward in time\n",
    "\n",
    "                    it = beam_seq_table[divm][:, :, t - divm].reshape(-1)\n",
    "                    logprobs_table[divm], state_table[divm] = self.get_logprobs_state(it.cuda(), *(\n",
    "                            args[divm] + [state_table[divm]]))\n",
    "                    logprobs_table[divm] = F.log_softmax(logprobs_table[divm] / temperature, dim=-1)\n",
    "\n",
    "        # all beams are sorted by their log-probabilities\n",
    "        done_beams_table = [[sorted(done_beams_table[b][i], key=lambda x: -x['p'])[:bdash] for i in range(group_size)]\n",
    "                            for b in range(batch_size)]\n",
    "        done_beams = [sum(_, []) for _ in done_beams_table]\n",
    "\n",
    "        # print(f'done_beams: {done_beams}')\n",
    "        log_message(f\"done_beams: {list(map(lambda x: (x['seq'], x['p']), done_beams[0]))}\")\n",
    "        return done_beams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "537bcc07-2db9-4bd7-8a0f-b1aad873f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention model\n",
    "class AttModel(CaptionModel):\n",
    "    def __init__(self, args, tokenizer):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.input_encoding_size = args.d_model\n",
    "        self.rnn_size = args.d_ff\n",
    "        self.num_layers = args.num_layers\n",
    "        self.drop_prob_lm = args.drop_prob_lm\n",
    "        self.max_seq_length = args.max_seq_length\n",
    "        self.att_feat_size = args.d_vf\n",
    "        self.att_hid_size = args.d_model\n",
    "\n",
    "        self.bos_idx = args.bos_idx\n",
    "        self.eos_idx = args.eos_idx\n",
    "        self.pad_idx = args.pad_idx\n",
    "\n",
    "        self.use_bn = args.use_bn\n",
    "\n",
    "        self.embed = lambda x: x\n",
    "        self.fc_embed = lambda x: x\n",
    "        self.att_embed = nn.Sequential(*(\n",
    "                ((nn.BatchNorm1d(self.att_feat_size),) if self.use_bn else ()) +\n",
    "                (nn.Linear(self.att_feat_size, self.input_encoding_size),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Dropout(self.drop_prob_lm)) +\n",
    "                ((nn.BatchNorm1d(self.input_encoding_size),) if self.use_bn == 2 else ())))\n",
    "\n",
    "        self.out1 = nn.Sequential(\n",
    "            nn.Linear(self.att_feat_size, 1024),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.out2 = nn.Sequential(\n",
    "            nn.Linear(self.rnn_size, self.rnn_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(self.att_feat_size)\n",
    "\n",
    "    def clip_att(self, att_feats, att_masks):\n",
    "        # Clip the length of att_masks and att_feats to the maximum length\n",
    "        if att_masks is not None:\n",
    "            max_len = att_masks.data.long().sum(1).max()\n",
    "            att_feats = att_feats[:, :max_len].contiguous()\n",
    "            att_masks = att_masks[:, :max_len].contiguous()\n",
    "        return att_feats, att_masks\n",
    "\n",
    "    def multimodal_feat(self, att_feats, meshes):# Concate multimodal features\n",
    "        return torch.cat((self.ln(att_feats),self.ln(meshes)),dim=1)\n",
    "        # return torch.cat((self.ln(self.out1(att_feats)),self.ln(self.out2(meshes))),dim=1)\n",
    "\n",
    "    def _prepare_feature(self, fc_feats, att_feats, att_masks):\n",
    "        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n",
    "\n",
    "        # embed fc and att feats\n",
    "        fc_feats = self.fc_embed(fc_feats)\n",
    "        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n",
    "\n",
    "        # Project the attention feats first to reduce memory and computation comsumptions.\n",
    "        p_att_feats = self.ctx2att(att_feats)\n",
    "\n",
    "        return fc_feats, att_feats, p_att_feats, att_masks\n",
    "\n",
    "    def get_logprobs_state(self, it, fc_feats, att_feats, p_att_feats, att_masks, state, output_logsoftmax=1):\n",
    "        # 'it' contains a word index\n",
    "        xt = self.embed(it)\n",
    "\n",
    "        output, state = self.core(xt, fc_feats, att_feats, p_att_feats, state, att_masks)\n",
    "        if output_logsoftmax:\n",
    "            logprobs = F.log_softmax(self.logit(output), dim=1)\n",
    "        else:\n",
    "            logprobs = self.logit(output)\n",
    "\n",
    "        return logprobs, state\n",
    "\n",
    "    def _sample_beam(self, fc_feats, att_feats, att_masks=None, meshes=None, opt={}):\n",
    "        # print(f'opt: {opt}')\n",
    "        beam_size = opt.get('beam_size', 10)\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        sample_n = opt.get('sample_n', 10)\n",
    "        # when sample_n == beam_size then each beam is a sample.\n",
    "        assert sample_n == 1 or sample_n == beam_size // group_size, 'when beam search, sample_n == 1 or beam search'\n",
    "        batch_size = fc_feats.size(0)\n",
    "\n",
    "        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks)\n",
    "\n",
    "        assert beam_size <= self.vocab_size + 1, 'lets assume this for now, otherwise this corner case causes a few headaches down the road. can be dealt with in future if needed'\n",
    "        seq = fc_feats.new_full((batch_size * sample_n, self.max_seq_length), self.pad_idx, dtype=torch.long)\n",
    "        seqLogprobs = fc_feats.new_zeros(batch_size * sample_n, self.max_seq_length, self.vocab_size + 1)\n",
    "        # lets process every image independently for now, for simplicity\n",
    "\n",
    "        self.done_beams = [[] for _ in range(batch_size)]\n",
    "\n",
    "        state = self.init_hidden(batch_size)\n",
    "\n",
    "        # first step, feed bos\n",
    "        it = fc_feats.new_full([batch_size], self.bos_idx, dtype=torch.long)\n",
    "        logprobs, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, state)\n",
    "\n",
    "        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = repeat_tensors(beam_size,\n",
    "                                                                                  [p_fc_feats, p_att_feats,\n",
    "                                                                                   pp_att_feats, p_att_masks]\n",
    "                                                                                  )\n",
    "        self.done_beams = self.beam_search(state, logprobs, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, opt=opt)\n",
    "        # print(f'self.done_beams: {self.done_beams}')\n",
    "        for k in range(batch_size):\n",
    "            if sample_n == beam_size:\n",
    "                for _n in range(sample_n):\n",
    "                    seq_len = self.done_beams[k][_n]['seq'].shape[0]\n",
    "                    seq[k * sample_n + _n, :seq_len] = self.done_beams[k][_n]['seq']\n",
    "                    seqLogprobs[k * sample_n + _n, :seq_len] = self.done_beams[k][_n]['logps']\n",
    "                    # print(f'---> seq: {seq}, seqLogprobs: {seqLogprobs}')\n",
    "            else:\n",
    "                seq_len = self.done_beams[k][0]['seq'].shape[0]\n",
    "                seq[k, :seq_len] = self.done_beams[k][0]['seq']  # the first beam has highest cumulative score\n",
    "                seqLogprobs[k, :seq_len] = self.done_beams[k][0]['logps']\n",
    "        # return the samples and their log likelihoods\n",
    "        # print(f'_sample_beam: seq {seq}, seqLogprobs {seqLogprobs}')\n",
    "        return seq, seqLogprobs\n",
    "\n",
    "    def _sample(self, fc_feats, att_feats, meshes=None, att_masks=None):\n",
    "        opt = self.args.__dict__\n",
    "        sample_method = opt.get('sample_method', 'greedy')\n",
    "        beam_size = opt.get('beam_size', 1)\n",
    "        temperature = opt.get('temperature', 1.0)\n",
    "        sample_n = int(opt.get('sample_n', 1))\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        output_logsoftmax = opt.get('output_logsoftmax', 1)\n",
    "        decoding_constraint = opt.get('decoding_constraint', 0)\n",
    "        block_trigrams = opt.get('block_trigrams', 0)\n",
    "        if beam_size > 1 and sample_method in ['greedy', 'beam_search']:\n",
    "            return self._sample_beam(fc_feats, att_feats, att_masks, meshes, opt)\n",
    "        if group_size > 1:\n",
    "            return self._diverse_sample(fc_feats, att_feats, att_masks, meshes, opt)\n",
    "\n",
    "        batch_size = fc_feats.size(0)\n",
    "        state = self.init_hidden(batch_size * sample_n)\n",
    "\n",
    "        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks, meshes)\n",
    "\n",
    "        if sample_n > 1:\n",
    "            p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = repeat_tensors(sample_n,\n",
    "                                                                                      [p_fc_feats, p_att_feats,\n",
    "                                                                                       pp_att_feats, p_att_masks]\n",
    "                                                                                      )\n",
    "\n",
    "        trigrams = []  # will be a list of batch_size dictionaries\n",
    "\n",
    "        seq = fc_feats.new_full((batch_size * sample_n, self.max_seq_length), self.pad_idx, dtype=torch.long)\n",
    "        seqLogprobs = fc_feats.new_zeros(batch_size * sample_n, self.max_seq_length, self.vocab_size + 1)\n",
    "        for t in range(self.max_seq_length + 1):\n",
    "            if t == 0:  # input <bos>\n",
    "                it = fc_feats.new_full([batch_size * sample_n], self.bos_idx, dtype=torch.long)\n",
    "\n",
    "            logprobs, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, state,\n",
    "                                                      output_logsoftmax=output_logsoftmax)\n",
    "\n",
    "            if decoding_constraint and t > 0:\n",
    "                tmp = logprobs.new_zeros(logprobs.size())\n",
    "                tmp.scatter_(1, seq[:, t - 1].data.unsqueeze(1), float('-inf'))\n",
    "                logprobs = logprobs + tmp\n",
    "\n",
    "            # Mess with trigrams\n",
    "            # Copy from https://github.com/lukemelas/image-paragraph-captioning\n",
    "            if block_trigrams and t >= 3:\n",
    "                # Store trigram generated at last step\n",
    "                prev_two_batch = seq[:, t - 3:t - 1]\n",
    "                for i in range(batch_size):  # = seq.size(0)\n",
    "                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                    current = seq[i][t - 1]\n",
    "                    if t == 3:  # initialize\n",
    "                        trigrams.append({prev_two: [current]})  # {LongTensor: list containing 1 int}\n",
    "                    elif t > 3:\n",
    "                        if prev_two in trigrams[i]:  # add to list\n",
    "                            trigrams[i][prev_two].append(current)\n",
    "                        else:  # create list\n",
    "                            trigrams[i][prev_two] = [current]\n",
    "                # Block used trigrams at next step\n",
    "                prev_two_batch = seq[:, t - 2:t]\n",
    "                mask = torch.zeros(logprobs.size(), requires_grad=False).cuda()  # batch_size x vocab_size\n",
    "                for i in range(batch_size):\n",
    "                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                    if prev_two in trigrams[i]:\n",
    "                        for j in trigrams[i][prev_two]:\n",
    "                            mask[i, j] += 1\n",
    "                # Apply mask to log probs\n",
    "                # logprobs = logprobs - (mask * 1e9)\n",
    "                alpha = 2.0  # = 4\n",
    "                logprobs = logprobs + (mask * -0.693 * alpha)  # ln(1/2) * alpha (alpha -> infty works best)\n",
    "\n",
    "            # sample the next word\n",
    "            if t == self.max_seq_length:  # skip if we achieve maximum length\n",
    "                break\n",
    "            it, sampleLogprobs = self.sample_next_word(logprobs, sample_method, temperature)\n",
    "\n",
    "            # stop when all finished\n",
    "            if t == 0:\n",
    "                unfinished = it != self.eos_idx\n",
    "            else:\n",
    "                it[~unfinished] = self.pad_idx  # This allows eos_idx not being overwritten to 0\n",
    "                logprobs = logprobs * unfinished.unsqueeze(1).float()\n",
    "                unfinished = unfinished * (it != self.eos_idx)\n",
    "            seq[:, t] = it\n",
    "            seqLogprobs[:, t] = logprobs\n",
    "            # quit loop if all sequences have finished\n",
    "            if unfinished.sum() == 0:\n",
    "                break\n",
    "\n",
    "        return seq, seqLogprobs\n",
    "\n",
    "    def _diverse_sample(self, fc_feats, att_feats, att_masks=None, opt={}):\n",
    "\n",
    "        sample_method = opt.get('sample_method', 'greedy')\n",
    "        beam_size = opt.get('beam_size', 1)\n",
    "        temperature = opt.get('temperature', 1.0)\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        diversity_lambda = opt.get('diversity_lambda', 0.5)\n",
    "        decoding_constraint = opt.get('decoding_constraint', 0)\n",
    "        block_trigrams = opt.get('block_trigrams', 0)\n",
    "\n",
    "        batch_size = fc_feats.size(0)\n",
    "        state = self.init_hidden(batch_size)\n",
    "\n",
    "        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks)\n",
    "\n",
    "        trigrams_table = [[] for _ in range(group_size)]  # will be a list of batch_size dictionaries\n",
    "\n",
    "        seq_table = [fc_feats.new_full((batch_size, self.max_seq_length), self.pad_idx, dtype=torch.long) for _ in\n",
    "                     range(group_size)]\n",
    "        seqLogprobs_table = [fc_feats.new_zeros(batch_size, self.max_seq_length) for _ in range(group_size)]\n",
    "        state_table = [self.init_hidden(batch_size) for _ in range(group_size)]\n",
    "\n",
    "        for tt in range(self.max_seq_length + group_size):\n",
    "            for divm in range(group_size):\n",
    "                t = tt - divm\n",
    "                seq = seq_table[divm]\n",
    "                seqLogprobs = seqLogprobs_table[divm]\n",
    "                trigrams = trigrams_table[divm]\n",
    "                if t >= 0 and t <= self.max_seq_length - 1:\n",
    "                    if t == 0:  # input <bos>\n",
    "                        it = fc_feats.new_full([batch_size], self.bos_idx, dtype=torch.long)\n",
    "                    else:\n",
    "                        it = seq[:, t - 1]  # changed\n",
    "\n",
    "                    logprobs, state_table[divm] = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats,\n",
    "                                                                          p_att_masks, state_table[divm])  # changed\n",
    "                    logprobs = F.log_softmax(logprobs / temperature, dim=-1)\n",
    "\n",
    "                    # Add diversity\n",
    "                    if divm > 0:\n",
    "                        unaug_logprobs = logprobs.clone()\n",
    "                        for prev_choice in range(divm):\n",
    "                            prev_decisions = seq_table[prev_choice][:, t]\n",
    "                            logprobs[:, prev_decisions] = logprobs[:, prev_decisions] - diversity_lambda\n",
    "\n",
    "                    if decoding_constraint and t > 0:\n",
    "                        tmp = logprobs.new_zeros(logprobs.size())\n",
    "                        tmp.scatter_(1, seq[:, t - 1].data.unsqueeze(1), float('-inf'))\n",
    "                        logprobs = logprobs + tmp\n",
    "\n",
    "                    # Mess with trigrams\n",
    "                    if block_trigrams and t >= 3:\n",
    "                        # Store trigram generated at last step\n",
    "                        prev_two_batch = seq[:, t - 3:t - 1]\n",
    "                        for i in range(batch_size):  # = seq.size(0)\n",
    "                            prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                            current = seq[i][t - 1]\n",
    "                            if t == 3:  # initialize\n",
    "                                trigrams.append({prev_two: [current]})  # {LongTensor: list containing 1 int}\n",
    "                            elif t > 3:\n",
    "                                if prev_two in trigrams[i]:  # add to list\n",
    "                                    trigrams[i][prev_two].append(current)\n",
    "                                else:  # create list\n",
    "                                    trigrams[i][prev_two] = [current]\n",
    "                        # Block used trigrams at next step\n",
    "                        prev_two_batch = seq[:, t - 2:t]\n",
    "                        mask = torch.zeros(logprobs.size(), requires_grad=False).cuda()  # batch_size x vocab_size\n",
    "                        for i in range(batch_size):\n",
    "                            prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                            if prev_two in trigrams[i]:\n",
    "                                for j in trigrams[i][prev_two]:\n",
    "                                    mask[i, j] += 1\n",
    "                        # Apply mask to log probs\n",
    "                        # logprobs = logprobs - (mask * 1e9)\n",
    "                        alpha = 2.0  # = 4\n",
    "                        logprobs = logprobs + (mask * -0.693 * alpha)  # ln(1/2) * alpha (alpha -> infty works best)\n",
    "\n",
    "                    it, sampleLogprobs = self.sample_next_word(logprobs, sample_method, 1)\n",
    "\n",
    "                    # stop when all finished\n",
    "                    if t == 0:\n",
    "                        unfinished = it != self.eos_idx\n",
    "                    else:\n",
    "                        unfinished = seq[:, t - 1] != self.pad_idx & seq[:, t - 1] != self.eos_idx\n",
    "                        it[~unfinished] = self.pad_idx\n",
    "                        unfinished = unfinished & (it != self.eos_idx)  # changed\n",
    "                    seq[:, t] = it\n",
    "                    seqLogprobs[:, t] = sampleLogprobs.view(-1)\n",
    "\n",
    "        return torch.stack(seq_table, 1).reshape(batch_size * group_size, -1), torch.stack(seqLogprobs_table,\n",
    "                                                                                           1).reshape(\n",
    "            batch_size * group_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e1ecac-b461-46e8-ac58-2ea31ffe5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbfaaa8-391a-452d-b1a1-fac5617ecd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N, PAM):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.d_model)\n",
    "        self.PAM = clones(PAM, N)\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        s=[]\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            x = layer(x, mask)\n",
    "            s.append(self.PAM[i](x))\n",
    "\n",
    "\n",
    "        o = s[0]\n",
    "        for i in range(1,len(s)):\n",
    "            o +=  s[i]\n",
    "        return o\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(d_model, dropout), 2)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0979b08d-c47a-4564-8014-b63cc956f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, self_attn, src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(d_model, dropout), 3)\n",
    "\n",
    "    def forward(self, x, hidden_states, src_mask, tgt_mask):\n",
    "        m = hidden_states\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.d_model)\n",
    "\n",
    "    def forward(self, x, hidden_states, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, hidden_states, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3c7a32-296c-4aa0-9821-064599c87fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, hidden_states, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), hidden_states, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        x, self.attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class PAM(nn.Module):\n",
    "    def __init__(self, dim=512):\n",
    "        super(PAM, self).__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim, 13, 1, 13//2, groups=dim)\n",
    "        self.proj1 = nn.Conv2d(dim, dim, 7, 1, 7//2, groups=dim)\n",
    "        self.proj2 = nn.Conv2d(dim, dim, 3, 1, 3//2, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, C = x.shape\n",
    "        assert int(math.sqrt(H))**2==H, f'{x.shape}'\n",
    "        cnn_feat = x.transpose(1, 2).view(B, C, int(math.sqrt(H)), int(math.sqrt(H))).contiguous()\n",
    "        x = self.proj(cnn_feat)+cnn_feat+self.proj1(cnn_feat)+self.proj2(cnn_feat)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderDecoder(AttModel):\n",
    "\n",
    "    def __init__(self, args, tokenizer):\n",
    "        super().__init__(args, tokenizer)\n",
    "        self.args = args\n",
    "        self.num_layers = args.num_layers\n",
    "        self.d_model = args.d_model\n",
    "        self.d_ff = args.d_ff\n",
    "        self.num_heads = args.num_heads\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        tgt_vocab = self.vocab_size + 1\n",
    "\n",
    "        self.embeded = Embeddings(args.d_vf, tgt_vocab)\n",
    "        self.model = self.__build_model(tgt_vocab)\n",
    "        self.__init_model()\n",
    "\n",
    "        self.logit = nn.Linear(args.d_model, tgt_vocab)\n",
    "        self.logit_mesh = nn.Linear(args.d_model, args.d_model)\n",
    "\n",
    "    def __build_model(self, tgt_vocab):\n",
    "        attn = MultiHeadedAttention(self.num_heads, self.d_model)\n",
    "        ff = PositionwiseFeedForward(self.d_model, self.d_ff, self.dropout)\n",
    "        position = PositionalEncoding(self.d_model, self.dropout)\n",
    "        pp = PAM(self.d_model)\n",
    "        model = Transformer(\n",
    "            Encoder(EncoderLayer(self.d_model, deepcopy(attn), deepcopy(ff), self.dropout), self.num_layers, pp),\n",
    "            Decoder(\n",
    "                DecoderLayer(self.d_model, deepcopy(attn), deepcopy(attn), deepcopy(ff), self.dropout),\n",
    "                self.num_layers),\n",
    "            lambda x: x,\n",
    "            nn.Sequential(Embeddings(self.d_model, tgt_vocab), deepcopy(position))\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        return []\n",
    "\n",
    "    def __init_model(self):\n",
    "        for p in self.model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def _prepare_feature(self, fc_feats, att_feats, att_masks, meshes=None):\n",
    "        att_feats = pad_tokens(att_feats)\n",
    "        att_feats, seq, _, att_masks, seq_mask, _ = self._prepare_feature_forward(att_feats, att_masks, meshes)\n",
    "        memory = self.model.encode(att_feats, att_masks)\n",
    "\n",
    "        return fc_feats[..., :1], att_feats[..., :1], memory, att_masks\n",
    "\n",
    "    def _prepare_feature_mesh(self, att_feats, att_masks=None, meshes=None):\n",
    "        att_feats = pad_tokens(att_feats)\n",
    "        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n",
    "        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n",
    "\n",
    "        if att_masks is None:\n",
    "            att_masks = att_feats.new_ones(att_feats.shape[:2], dtype=torch.long)\n",
    "        att_masks = att_masks.unsqueeze(-2)\n",
    "\n",
    "        if meshes is not None:\n",
    "            # crop the last one\n",
    "            meshes = meshes[:, :-1]\n",
    "            meshes_mask = (meshes.data > 0)\n",
    "            meshes_mask[:, 0] += True\n",
    "\n",
    "            meshes_mask = meshes_mask.unsqueeze(-2)\n",
    "            meshes_mask = meshes_mask & subsequent_mask(meshes.size(-1)).to(meshes_mask)\n",
    "        else:\n",
    "            meshes_mask = None\n",
    "\n",
    "        return att_feats, meshes, att_masks, meshes_mask\n",
    "\n",
    "    def _prepare_feature_forward(self, att_feats, att_masks=None, meshes=None, seq=None):\n",
    "\n",
    "        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n",
    "        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n",
    "\n",
    "        if att_masks is None:\n",
    "            att_masks = att_feats.new_ones(att_feats.shape[:2], dtype=torch.long)\n",
    "        att_masks = att_masks.unsqueeze(-2)\n",
    "\n",
    "        if seq is not None:\n",
    "            # crop the last one\n",
    "            seq = seq[:, :-1]\n",
    "            seq_mask = (seq.data > 0)\n",
    "            seq_mask[:, 0] += True\n",
    "\n",
    "            seq_mask = seq_mask.unsqueeze(-2)\n",
    "            seq_mask = seq_mask & subsequent_mask(seq.size(-1)).to(seq_mask)\n",
    "        else:\n",
    "            seq_mask = None\n",
    "\n",
    "        if meshes is not None:\n",
    "            # crop the last one\n",
    "            meshes = meshes[:, :-1]\n",
    "            meshes_mask = (meshes.data > 0)\n",
    "            meshes_mask[:, 0] += True\n",
    "\n",
    "            meshes_mask = meshes_mask.unsqueeze(-2)\n",
    "            meshes_mask = meshes_mask & subsequent_mask(meshes.size(-1)).to(meshes_mask)\n",
    "        else:\n",
    "            meshes_mask = None\n",
    "\n",
    "        return att_feats, seq, meshes, att_masks, seq_mask, meshes_mask\n",
    "\n",
    "    def _forward(self, fc_feats, att_feats, report_ids, att_masks=None):\n",
    "        # log_message(fc_feats, att_feats, report_ids, att_masks)\n",
    "        att_feats, report_ids, att_masks, report_mask = self._prepare_feature_mesh(att_feats, att_masks, report_ids)\n",
    "        out = self.model(att_feats, report_ids, att_masks, report_mask)\n",
    "\n",
    "        # print(f'out: {out}')\n",
    "        outputs = F.log_softmax(self.logit(out), dim=-1)\n",
    "        # print(f'outputs: {outputs}')\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def core(self, it, fc_feats_ph, att_feats_ph, memory, state, mask):\n",
    "\n",
    "        if len(state) == 0:\n",
    "            ys = it.long().unsqueeze(1)\n",
    "        else:\n",
    "            ys = torch.cat([state[0][0], it.unsqueeze(1)], dim=1)\n",
    "        out = self.model.decode(memory, mask, ys, subsequent_mask(ys.size(1)).to(memory.device))\n",
    "        return out[:, -1], [ys.unsqueeze(0)]\n",
    "\n",
    "    def _encode(self, fc_feats, att_feats, att_masks=None):\n",
    "\n",
    "        att_feats, _, att_masks, _ = self._prepare_feature_mesh(att_feats, att_masks)\n",
    "        out = self.model.encode(att_feats, att_masks)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1669a405-5836-4fc2-9b46-e78231e64126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReportGenModel(nn.Module):\n",
    "\n",
    "    def __init__(self, args, tokenizer):\n",
    "        super().__init__()\n",
    "        self.__tokenizer = tokenizer\n",
    "\n",
    "        self.prompt = nn.Parameter(torch.randn(1, 1, args.d_vf))\n",
    "\n",
    "\n",
    "        self.positional_encoder = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, args.d_vf)\n",
    "        )\n",
    "\n",
    "        self.encoder_decoder = EncoderDecoder(args, tokenizer)\n",
    "\n",
    "\n",
    "    def forward(self, image_embeddings, pos_embeddings, report_ids, patch_masks, mode='train'):\n",
    "        # print(f'image_embeddings: {image_embeddings.shape}, pos_embeddings: {pos_embeddings.shape}')\n",
    "        coords_encoded = self.positional_encoder(pos_embeddings)\n",
    "        patch_feats = image_embeddings + coords_encoded\n",
    "        # print(self.prompt.shape, patch_feats.shape)\n",
    "        att_feats = torch.cat([self.prompt, patch_feats], dim=1)\n",
    "        \n",
    "        fc_feats = torch.sum(att_feats, dim=1)\n",
    "        if mode == 'train':\n",
    "            output = self.encoder_decoder(fc_feats, att_feats, report_ids, mode='forward')\n",
    "        elif mode == 'sample':\n",
    "            output, _ = self.encoder_decoder(fc_feats, att_feats, mode='sample')\n",
    "        elif mode == 'encode':\n",
    "            output = self.encoder_decoder(fc_feats, att_feats, mode='encode')\n",
    "\n",
    "            logits = self.fc(output[0,0,:]).unsqueeze(0)\n",
    "            Y_hat = torch.argmax(logits, dim=1)\n",
    "            Y_prob = F.softmax(logits, dim=1)\n",
    "            return Y_hat, Y_prob\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c952eb18-e13b-4640-ace8-10d3f91e0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        # truncate to the same size\n",
    "        target = target[:, :input.size(1)]\n",
    "        mask = mask[:, :input.size(1)]\n",
    "        output = -input.gather(2, target.long().unsqueeze(2)).squeeze(2) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a158325-a3f6-47e6-a049-fe0de679b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "class ReportModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, tokenizer, weight_decay=0.01):\n",
    "        super().__init__()\n",
    "        self.model = ReportGenModel(args, tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.__lr = args.lr\n",
    "        self.__weight_decay = weight_decay\n",
    "        self.rouge = ROUGEScore()\n",
    "        self.bleu = BLEUScore(n_gram=1)\n",
    "\n",
    "    def loss_fn(self, output, reports_ids, reports_masks):\n",
    "        \n",
    "        criterion = LanguageModelCriterion()\n",
    "        loss = criterion(output, reports_ids[:, 1:], reports_masks[:, 1:]).mean()\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        # print('train ---------->')\n",
    "        _, patch_feats, pos_feats, report_ids, report_masks, patch_masks = batch\n",
    "        output = self.model(patch_feats, pos_feats, report_ids, patch_masks, mode='train')\n",
    "        # print(f'train output: {output}')\n",
    "        loss = self.loss_fn(output, report_ids, report_masks)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # print('val ---------->')\n",
    "        _, patch_feats, pos_feats, report_ids, report_masks, patch_masks = batch\n",
    "        \n",
    "        output_ = self.model(patch_feats, pos_feats, report_ids, patch_masks, mode='train')\n",
    "        # print(f'val output: {output_}')\n",
    "        loss = self.loss_fn(output_, report_ids, report_masks)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        \n",
    "        if batch_idx%100==0:\n",
    "            output = self.model(patch_feats, pos_feats, report_ids,patch_masks, mode='sample')\n",
    "            pred_texts = self.tokenizer.batch_decode(output.cpu().numpy())\n",
    "            target_texts = self.tokenizer.batch_decode(report_ids[:, 1:].cpu().numpy())\n",
    "            # print(f'val output: {output_}')\n",
    "            # print(f'report_ids: {report_ids}, output: {output}')\n",
    "            print(f'pred_texts: {pred_texts}, target_texts: {target_texts}')\n",
    "\n",
    "            rouge_score = self.rouge(pred_texts, target_texts)\n",
    "            bleu_score = self.bleu(pred_texts, target_texts)\n",
    "    \n",
    "            self.log('val_rouge', rouge_score['rouge1_fmeasure'], on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "            self.log('val_bleu', bleu_score, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        d_params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = torch.optim.AdamW(d_params, lr=self.__lr, weight_decay=self.__weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08be2c94-434e-46f4-996b-9183f741ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datamodule\n",
    "\n",
    "class PatchEmbeddingDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,args, tokenizer, split_frac, shuffle = False):\n",
    "        super().__init__()\n",
    "        self.test_ds = None\n",
    "        self.val_ds = None\n",
    "        self.train_ds = None\n",
    "        self.__batch_size = args.batch_size\n",
    "        self.__shuffle = shuffle\n",
    "        self.__num_workers = args.num_workers\n",
    "        self.__embeddings_path = args.embeddings_path\n",
    "        self.__reports_json_path = args.reports_json_path\n",
    "        self.__max_seq_length = args.max_seq_length\n",
    "        self.__split_frac = split_frac\n",
    "        self.__tokenizer = tokenizer\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = EmbeddingDataset(self.__embeddings_path, self.__reports_json_path, self.__tokenizer,\n",
    "                              self.__max_seq_length)\n",
    "        self.train_ds, self.val_ds, self.test_ds = random_split(dataset, self.__split_frac)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.__batch_size, shuffle=self.__shuffle, collate_fn = self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.__batch_size, collate_fn = self.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.__batch_size, collate_fn = self.collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        slide_ids, patch_feats, coord_feats, report_ids, report_masks, seq_length = zip(*batch)\n",
    "        patch_feats_pad = pad_sequence(patch_feats, batch_first=True)\n",
    "        coord_feats_pad =  pad_sequence(coord_feats, batch_first=True)\n",
    "        patch_mask = torch.zeros(patch_feats_pad.shape[:2], dtype=torch.float32)\n",
    "        for i, p in enumerate(patch_feats):\n",
    "            patch_mask[i, :p.shape[0]] = 1\n",
    "\n",
    "        return (slide_ids, patch_feats_pad, coord_feats_pad, torch.LongTensor(report_ids),\n",
    "                torch.FloatTensor(report_masks), torch.FloatTensor(patch_mask))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb2e293-49ff-4cd2-a1fd-17a0881d78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, args, model, tokenizer, split_frac=(0.75, 0.12, 0.13)):\n",
    "        self.ckpt_path = args.ckpt_path\n",
    "        self.max_epochs = args.max_epochs\n",
    "        self.split_frac = split_frac\n",
    "        self.datamodule = PatchEmbeddingDataModule(args, tokenizer, split_frac)\n",
    "        self.model = model\n",
    "        pl.seed_everything(42)\n",
    "\n",
    "    def train(self, fast_dev_run=False):\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=self.ckpt_path,  # Directory to save checkpoints\n",
    "            filename=\"best_model\",  # Naming convention\n",
    "            monitor=\"val_loss\",  # Metric to monitor for saving best checkpoints\n",
    "            mode=\"min\",  # Whether to minimize or maximize the monitored metric\n",
    "            save_top_k=1,  # Number of best checkpoints to keep\n",
    "            save_last=True  # Save the last checkpoint regardless of the monitored metric\n",
    "        )\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=3, verbose=True, mode=\"min\")\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            accelerator='gpu',\n",
    "            devices=[2],\n",
    "            strategy='auto',\n",
    "            enable_progress_bar=True,\n",
    "            log_every_n_steps=2,\n",
    "            fast_dev_run=fast_dev_run\n",
    "        )\n",
    "        train_metrics = trainer.fit(\n",
    "            self.model, datamodule=self.datamodule\n",
    "        )\n",
    "\n",
    "        return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4eb77f61-e3a7-4bcc-a07d-1277d7202145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'max_fea_length': 10000,\n",
    "    'max_seq_length': 200,\n",
    "    'threshold': 1,\n",
    "    'batch_size': 1,\n",
    "    'ckpt_path': 'checkpoints/1',\n",
    "    'max_epochs': 100,\n",
    "    'd_ff': 512,\n",
    "    'd_vf': 768,\n",
    "    'd_model': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 10,\n",
    "    'dropout': 0.1,\n",
    "    'drop_prob_lm': 0.5,\n",
    "    'bos_idx': 0,\n",
    "    'eos_idx': 0,\n",
    "    'pad_idx': 0,\n",
    "    'use_bn': 0,\n",
    "    'beam_size': 3,\n",
    "    'num_workers': 2,\n",
    "    'embeddings_path': embeddings_path,\n",
    "    'reports_json_path': REPORTS_JSON_PATH,\n",
    "    'sample_n': 1,\n",
    "    'group_size': 1,\n",
    "    'lr': 1e-6,\n",
    "    'sample_method': 'beam_search',\n",
    "    'temperature':1.0,\n",
    "    'output_logsoftmax': 1,\n",
    "    'decoding_constraint': 1,\n",
    "    'suppress_UNK': 1,\n",
    "    'block_trigrams': 1,\n",
    "    'length_penalty': 'wu_0.8'\n",
    "    \n",
    "}\n",
    "debug = False\n",
    "args = argparse.Namespace(**config)\n",
    "split_frac = [0.85, 0.10, 0.05]\n",
    "model = ReportModel(args, tokenizer)\n",
    "\n",
    "trainer = Trainer(args, model, tokenizer, split_frac)\n",
    "# metrics = trainer.train(fast_dev_run=False)\n",
    "# print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f87860e-8a58-49fd-9ae2-14403a227bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(model_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1012b01-6b8a-48e7-84b8-83972bed4eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = model.state_dict()\n",
    "state_dict = {k:v for k,v in ckpt.items() if k in model_dict}\n",
    "model_dict.update(state_dict) \n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f75481d-3d1e-4bc6-811d-fea1fe341e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name  | Type           | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model | ReportGenModel | 45.7 M | train\n",
      "1 | rouge | ROUGEScore     | 0      | train\n",
      "2 | bleu  | BLEUScore      | 0      | train\n",
      "-------------------------------------------------\n",
      "45.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "45.7 M    Total params\n",
      "182.913   Total estimated model params size (MB)\n",
      "556       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9523178ff7a0498b9fcfa650e55b18a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: ['45% 90%), marginal granuloma marginal 1 marginal (Gleason marginal (Gleason marginal Invasive Micropapillary marginal favor (Gleason marginal 1 marginal 1 marginal (Gleason marginal (Gleason marginal (Gleason marginal 45% 90%), granuloma stromal 1 Sessile ulcer granuloma stromal (Gleason marginal 1 marginal 1 marginal 1 marginal 1 marginal 1 marginal 1 marginal 1 marginal 1 marginal 1 (Gleason Micropapillary marginal ulcer Micropapillary marginal hamartoma Micropapillary marginal Invasive marginal mastitis favor hamartoma stromal 1 (Gleason marginal 1 (Gleason marginal (3+5), Micropapillary marginal 1 marginal 1 (Gleason favor hamartoma (LSIL; (Gleason marginal 1 marginal hamartoma (Gleason marginal hamartoma Micropapillary marginal hamartoma Micropapillary marginal sono-guided marginal 45% Micropapillary marginal favor hamartoma marginal Invasive marginal 45% Micropapillary marginal (Gleason marginal hamartoma (Gleason marginal hamartoma marginal hamartoma Micropapillary marginal Invasive fibrosis Micropapillary marginal (Gleason Micropapillary marginal (3+5), stromal marginal Invasive Micropapillary marginal Invasive marginal Invasive marginal Invasive marginal Fibrocystic (3+5), granuloma marginal hamartoma granuloma marginal 1 (Gleason Micropapillary marginal 1 (Gleason Micropapillary marginal 45% marginal 1 (Gleason favor Micropapillary marginal (3+5), Micropapillary marginal Micropapillary favor (Gleason Micropapillary marginal 45% Micropapillary marginal 1 (Gleason marginal 1 marginal 1 marginal (3+5), marginal (3+5), marginal (Gleason granuloma marginal Invasive Micropapillary marginal (3+5), marginal Invasive Micropapillary marginal 45% 90%), Micropapillary marginal'], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286c5aede4004513afb27bda8f103cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae653ddf9f242f0be243f8c37466af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: [''], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.532\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB. GPU 2 has a total capacity of 44.47 GiB of which 228.38 MiB is free. Including non-PyTorch memory, this process has 44.25 GiB memory in use. Of the allocated memory 41.06 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(args, model, tokenizer, split_frac)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_dev_run\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(metrics)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, fast_dev_run)\u001b[39m\n\u001b[32m     22\u001b[39m early_stop_callback = EarlyStopping(monitor=\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, min_delta=\u001b[32m1e-4\u001b[39m, patience=\u001b[32m3\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, mode=\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m trainer = pl.Trainer(\n\u001b[32m     24\u001b[39m     max_epochs=\u001b[38;5;28mself\u001b[39m.max_epochs,\n\u001b[32m     25\u001b[39m     callbacks=[checkpoint_callback, early_stop_callback],\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     fast_dev_run=fast_dev_run\n\u001b[32m     32\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m train_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m train_metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    538\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    569\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    570\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    571\u001b[39m     ckpt_path,\n\u001b[32m    572\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    573\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    977\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    987\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1026\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1025\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/trainer/call.py:171\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    174\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/core/module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1273\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1276\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1277\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/torch/optim/lr_scheduler.py:140\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m opt = opt_ref()\n\u001b[32m    139\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/torch/optim/adamw.py:220\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    223\u001b[39m     params_with_grad: List[Tensor] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_closure\u001b[39m(\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     98\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     optimizer: Steppable,\n\u001b[32m    100\u001b[39m     closure: Callable[[], Any],\n\u001b[32m    101\u001b[39m ) -> Any:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    hook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m._zero_grad_fn()\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:241\u001b[39m, in \u001b[36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[39m\u001b[34m(loss)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_fn\u001b[39m(loss: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/trainer/call.py:323\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    326\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:213\u001b[39m, in \u001b[36mStrategy.backward\u001b[39m\u001b[34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    211\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.pre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.post_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n\u001b[32m    216\u001b[39m \u001b[38;5;28mself\u001b[39m.post_backward(closure_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:73\u001b[39m, in \u001b[36mPrecision.backward\u001b[39m\u001b[34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     **kwargs: Any,\n\u001b[32m     61\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     62\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/pytorch_lightning/core/module.py:1097\u001b[39m, in \u001b[36mLightningModule.backward\u001b[39m\u001b[34m(self, loss, *args, **kwargs)\u001b[39m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28mself\u001b[39m._fabric.backward(loss, *args, **kwargs)\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/virtual_envs/master/lib64/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 2 has a total capacity of 44.47 GiB of which 228.38 MiB is free. Including non-PyTorch memory, this process has 44.25 GiB memory in use. Of the allocated memory 41.06 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(args, model, tokenizer, split_frac)\n",
    "metrics = trainer.train(fast_dev_run=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52c180-de4d-453a-835e-f65d40566597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756b8a6-4fe7-45f4-b52e-2aaacea471e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e71cef-8fc7-4b53-8629-32344eadc23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
