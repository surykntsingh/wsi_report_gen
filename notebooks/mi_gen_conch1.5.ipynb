{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998e4abf-f34c-431e-82a7-bfa175f06ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, PackedSequence, pack_padded_sequence, pad_packed_sequence\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.text.bleu import BLEUScore\n",
    "\n",
    "from copy import deepcopy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a848a4-e40b-4d42-882a-811526c7dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH = '/data04/shared/skapse/REG_processed/20x_512px_0px_overlap'\n",
    "REPORTS_JSON_PATH = '/home/sbsingh/projects/report_gen/train.json'\n",
    "model_ckpt_path = '/home/sbsingh/projects/report_gen/pretrained/model_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f67489-27fe-4a1a-9d20-05379e55cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(*message):\n",
    "    if debug:\n",
    "        print('_______'.join(message))\n",
    "\n",
    "def read_json_file(json_path):\n",
    "        with open(json_path) as f:\n",
    "            d = json.load(f)\n",
    "        return d\n",
    "\n",
    "def penalty_builder(penalty_config):\n",
    "    if penalty_config == '':\n",
    "        return lambda x, y: y\n",
    "    pen_type, alpha = penalty_config.split('_')\n",
    "    alpha = float(alpha)\n",
    "    if pen_type == 'wu':\n",
    "        return lambda x, y: length_wu(x, y, alpha)\n",
    "    if pen_type == 'avg':\n",
    "        return lambda x, y: length_average(x, y, alpha)\n",
    "\n",
    "\n",
    "def length_wu(length, logprobs, alpha=0.):\n",
    "    \"\"\"\n",
    "    NMT length re-ranking score from\n",
    "    \"Google's Neural Machine Translation System\" :cite:`wu2016google`.\n",
    "    \"\"\"\n",
    "\n",
    "    modifier = (((5 + length) ** alpha) /\n",
    "                ((5 + 1) ** alpha))\n",
    "    return logprobs / modifier\n",
    "\n",
    "\n",
    "def length_average(length, logprobs, alpha=0.):\n",
    "    \"\"\"\n",
    "    Returns the average probability of tokens in a sequence.\n",
    "    \"\"\"\n",
    "    if length<alpha:\n",
    "        penalty= -1000\n",
    "    else:\n",
    "        penalty = logprobs / length\n",
    "    log_message(f'length: {length}, logprobs: {logprobs}, penalty: {penalty}')\n",
    "    return penalty\n",
    "\n",
    "\n",
    "def split_tensors(n, x):\n",
    "    if torch.is_tensor(x):\n",
    "        assert x.shape[0] % n == 0\n",
    "        x = x.reshape(x.shape[0] // n, n, *x.shape[1:]).unbind(1)\n",
    "    elif type(x) is list or type(x) is tuple:\n",
    "        x = [split_tensors(n, _) for _ in x]\n",
    "    elif x is None:\n",
    "        x = [None] * n\n",
    "    return x\n",
    "\n",
    "\n",
    "def repeat_tensors(n, x):\n",
    "    \"\"\"\n",
    "    For a tensor of size Bx..., we repeat it n times, and make it Bnx...\n",
    "    For collections, do nested repeat\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.unsqueeze(1)  # Bx1x...\n",
    "        x = x.expand(-1, n, *([-1] * len(x.shape[2:])))  # Bxnx...\n",
    "        x = x.reshape(x.shape[0] * n, *x.shape[2:])  # Bnx...\n",
    "    elif type(x) is list or type(x) is tuple:\n",
    "        x = [repeat_tensors(n, _) for _ in x]\n",
    "    return x\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def pad_tokens(att_feats):\n",
    "    # ---->pad\n",
    "    H = att_feats.shape[1]\n",
    "    _H, _W = int(np.ceil(np.sqrt(H))), int(np.ceil(np.sqrt(H)))\n",
    "    add_length = _H * _W - H\n",
    "    att_feats = torch.cat([att_feats, att_feats[:, :add_length, :]], dim=1)  # [B, N, L]\n",
    "    return att_feats\n",
    "\n",
    "def sort_pack_padded_sequence(input, lengths):\n",
    "    lengths =lengths.cpu()\n",
    "    # log_message(f'lengths: {lengths}')\n",
    "    sorted_lengths, indices = torch.sort(lengths, descending=True)\n",
    "    # log_message(input[indices], sorted_lengths)\n",
    "    tmp = pack_padded_sequence(input[indices], sorted_lengths, batch_first=True)\n",
    "    inv_ix = indices.clone()\n",
    "    inv_ix[indices] = torch.arange(0, len(indices)).type_as(inv_ix)\n",
    "    return tmp, inv_ix\n",
    "\n",
    "\n",
    "def pad_unsort_packed_sequence(input, inv_ix):\n",
    "    tmp, _ = pad_packed_sequence(input, batch_first=True)\n",
    "    tmp = tmp[inv_ix]\n",
    "    return tmp\n",
    "\n",
    "def pack_wrapper(module, att_feats, att_masks):\n",
    "    # print(module, att_feats, att_masks)\n",
    "    if att_masks is not None:\n",
    "        packed, inv_ix = sort_pack_padded_sequence(att_feats, att_masks.data.long().sum(1))\n",
    "        return pad_unsort_packed_sequence(PackedSequence(module(packed[0]), packed[1]), inv_ix)\n",
    "    else:\n",
    "        return module(att_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1972a8f-9199-4cfd-bfd0-799c845a77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, reports_json_path, threshold = 1):\n",
    "        self.__threshold = threshold\n",
    "        self.__token2idx, self.__idx2token = self.create_vocabulary(reports_json_path)\n",
    "\n",
    "\n",
    "    def create_vocabulary(self, reports_json_path):\n",
    "        total_tokens = []\n",
    "        reports = read_json_file(reports_json_path)\n",
    "\n",
    "        for report in reports:\n",
    "            tokens = report['report'].split()\n",
    "            # for token in tokens:\n",
    "            #     total_tokens.append(token)\n",
    "            total_tokens.extend(tokens)\n",
    "\n",
    "        counter = Counter(total_tokens)\n",
    "        vocab = [k for k, v in counter.items() if v >= self.__threshold] + ['<unk>']\n",
    "        vocab.sort()\n",
    "        token2idx, idx2token = {}, {}\n",
    "        for idx, token in enumerate(vocab):\n",
    "            token2idx[token] = idx + 1\n",
    "            idx2token[idx + 1] = token\n",
    "\n",
    "        return token2idx, idx2token\n",
    "\n",
    "    def get_token_by_id(self, id):\n",
    "        return self.__idx2token[id]\n",
    "\n",
    "    def get_id_by_token(self, token):\n",
    "        if token not in self.__token2idx:\n",
    "            return self.__token2idx['<unk>']\n",
    "        return self.__token2idx[token]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.__token2idx)\n",
    "\n",
    "    def __call__(self, report):\n",
    "        tokens = report.split()\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.get_id_by_token(token))\n",
    "        ids = [0] + ids + [0]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # print(ids)\n",
    "        txt = ''\n",
    "        for i, idx in enumerate(ids):\n",
    "            if idx > 0:\n",
    "                if i >= 1:\n",
    "                    txt += ' '\n",
    "                txt += self.get_token_by_id(idx)\n",
    "            else:\n",
    "                break\n",
    "        return txt\n",
    "\n",
    "    def batch_decode(self, ids_batch):\n",
    "        # print(f'ids_batch: {ids_batch}, {ids_batch.shape}')\n",
    "        out = []\n",
    "        for ids in ids_batch:\n",
    "            out.append(self.decode(ids))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568c4b94-4a40-42c8-9aa0-1d70134f2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, embeddings_path, reports_json_path, tokenizer, max_seq_length):\n",
    "        reports = read_json_file(reports_json_path)\n",
    "        self.__reports = {report['id'].split('.')[0]: report['report'] for report in reports}\n",
    "        self.__tokenizer = tokenizer\n",
    "        self.__embeddings_path = embeddings_path\n",
    "        self.__max_seq_length = max_seq_length\n",
    "\n",
    "        files = os.listdir(embeddings_path)\n",
    "        self.__slides = [file.split('.')[0] for file in files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__slides)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id = self.__slides[idx]\n",
    "        with h5py.File(f'{self.__embeddings_path}/{slide_id}.h5', \"r\") as h5_file:\n",
    "            coords_np = h5_file[\"coords\"][:]\n",
    "            embeddings_np = h5_file[\"features\"][:]\n",
    "\n",
    "            coords = torch.tensor(coords_np).float() \n",
    "            embedding = torch.tensor(embeddings_np)\n",
    "            report_text = self.__reports[slide_id]\n",
    "            report_ids = self.__tokenizer(report_text)\n",
    "\n",
    "            if len(report_ids) < self.__max_seq_length:\n",
    "                padding = [0] * (self.__max_seq_length-len(report_ids))\n",
    "                report_ids.extend(padding)\n",
    "\n",
    "            report_masks = [1] * len(report_ids)\n",
    "            seq_length = len(report_ids)\n",
    "\n",
    "\n",
    "        return slide_id, embedding, coords, report_ids, report_masks, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfcc13fe-77ae-4431-8c23-5d8d3cfa196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(REPORTS_JSON_PATH)\n",
    "embeddings_path = f'{EMB_PATH}/features_conch_v15'\n",
    "max_seq_length = 300\n",
    "embedding_dataset = EmbeddingDataset(embeddings_path, REPORTS_JSON_PATH, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36ba08f-6ae4-479a-95e6-6e4cfb205686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PIT_01_02983_01',\n",
       " tensor([[ 0.1380,  0.5441, -1.0661,  ...,  1.4840, -1.2144, -0.5514],\n",
       "         [ 1.1093,  0.8226, -2.6591,  ...,  1.5781, -0.9008, -0.4615],\n",
       "         [ 1.2667,  0.8689, -2.5663,  ...,  1.6840, -0.2095,  0.2890],\n",
       "         ...,\n",
       "         [-1.5927,  0.4193, -2.3413,  ...,  2.2291, -1.5176, -0.0998],\n",
       "         [-0.4034,  0.9471, -1.8775,  ...,  2.4828, -1.8006,  0.1766],\n",
       "         [-0.6539, -0.3768, -1.6462,  ...,  2.0273, -0.9711, -0.4058]]),\n",
       " tensor([[  512., 12288.],\n",
       "         [ 1024., 11776.],\n",
       "         [ 1024., 12288.],\n",
       "         [ 1024., 12800.],\n",
       "         [ 1024., 13312.],\n",
       "         [ 1024., 13824.],\n",
       "         [ 1024., 14336.],\n",
       "         [ 1536., 10240.],\n",
       "         [ 1536., 10752.],\n",
       "         [ 1536., 11264.],\n",
       "         [ 1536., 11776.],\n",
       "         [ 1536., 12288.],\n",
       "         [ 1536., 12800.],\n",
       "         [ 1536., 13312.],\n",
       "         [ 1536., 13824.],\n",
       "         [ 1536., 14336.],\n",
       "         [ 2048., 10752.],\n",
       "         [ 2048., 11264.],\n",
       "         [ 2048., 11776.],\n",
       "         [ 2048., 12288.],\n",
       "         [ 2048., 12800.],\n",
       "         [ 2048., 13312.],\n",
       "         [ 2048., 13824.],\n",
       "         [ 2048., 14336.],\n",
       "         [ 2560.,  3584.],\n",
       "         [ 2560.,  4096.],\n",
       "         [ 2560.,  4608.],\n",
       "         [ 2560., 11264.],\n",
       "         [ 2560., 11776.],\n",
       "         [ 2560., 12288.],\n",
       "         [ 2560., 12800.],\n",
       "         [ 2560., 13312.],\n",
       "         [ 2560., 13824.],\n",
       "         [ 2560., 15360.],\n",
       "         [ 2560., 15872.],\n",
       "         [ 3072.,  3072.],\n",
       "         [ 3072.,  3584.],\n",
       "         [ 3072.,  4096.],\n",
       "         [ 3072.,  4608.],\n",
       "         [ 3072., 11264.],\n",
       "         [ 3072., 11776.],\n",
       "         [ 3072., 12288.],\n",
       "         [ 3072., 12800.],\n",
       "         [ 3072., 13312.],\n",
       "         [ 3072., 14848.],\n",
       "         [ 3072., 15360.],\n",
       "         [ 3072., 15872.],\n",
       "         [ 3584.,  1536.],\n",
       "         [ 3584.,  2048.],\n",
       "         [ 3584.,  2560.],\n",
       "         [ 3584.,  3072.],\n",
       "         [ 3584.,  3584.],\n",
       "         [ 3584.,  4096.],\n",
       "         [ 3584.,  7168.],\n",
       "         [ 3584.,  7680.],\n",
       "         [ 3584., 11264.],\n",
       "         [ 3584., 11776.],\n",
       "         [ 3584., 12288.],\n",
       "         [ 3584., 12800.],\n",
       "         [ 3584., 13312.],\n",
       "         [ 4096.,  1024.],\n",
       "         [ 4096.,  1536.],\n",
       "         [ 4096.,  2048.],\n",
       "         [ 4096.,  2560.],\n",
       "         [ 4096.,  3072.],\n",
       "         [ 4096.,  3584.],\n",
       "         [ 4096.,  4096.],\n",
       "         [ 4096.,  6656.],\n",
       "         [ 4096.,  7168.],\n",
       "         [ 4096.,  7680.],\n",
       "         [ 4096., 11264.],\n",
       "         [ 4096., 11776.],\n",
       "         [ 4096., 12288.],\n",
       "         [ 4096., 15360.],\n",
       "         [ 4096., 15872.],\n",
       "         [ 4096., 16384.],\n",
       "         [ 4096., 16896.],\n",
       "         [ 4608.,  1536.],\n",
       "         [ 4608.,  2048.],\n",
       "         [ 4608.,  2560.],\n",
       "         [ 4608.,  3072.],\n",
       "         [ 4608.,  3584.],\n",
       "         [ 4608.,  4096.],\n",
       "         [ 4608.,  6656.],\n",
       "         [ 4608.,  7168.],\n",
       "         [ 4608.,  7680.],\n",
       "         [ 4608., 14848.],\n",
       "         [ 4608., 15360.],\n",
       "         [ 4608., 15872.],\n",
       "         [ 4608., 16384.],\n",
       "         [ 4608., 16896.],\n",
       "         [ 4608., 17408.],\n",
       "         [ 5120.,  2048.],\n",
       "         [ 5120.,  2560.],\n",
       "         [ 5120.,  3072.],\n",
       "         [ 5120.,  3584.],\n",
       "         [ 5120.,  4096.],\n",
       "         [ 5120.,  6656.],\n",
       "         [ 5120.,  7168.],\n",
       "         [ 5120.,  7680.],\n",
       "         [ 5120., 10240.],\n",
       "         [ 5120., 10752.],\n",
       "         [ 5120., 14848.],\n",
       "         [ 5120., 15360.],\n",
       "         [ 5120., 15872.],\n",
       "         [ 5120., 16384.],\n",
       "         [ 5120., 16896.],\n",
       "         [ 5120., 17408.],\n",
       "         [ 5632.,  2048.],\n",
       "         [ 5632.,  2560.],\n",
       "         [ 5632.,  3072.],\n",
       "         [ 5632.,  3584.],\n",
       "         [ 5632.,  4096.],\n",
       "         [ 5632.,  6656.],\n",
       "         [ 5632.,  7168.],\n",
       "         [ 5632.,  9216.],\n",
       "         [ 5632.,  9728.],\n",
       "         [ 5632., 10240.],\n",
       "         [ 5632., 10752.],\n",
       "         [ 5632., 11264.],\n",
       "         [ 5632., 12288.],\n",
       "         [ 5632., 12800.],\n",
       "         [ 5632., 15360.],\n",
       "         [ 5632., 15872.],\n",
       "         [ 5632., 16384.],\n",
       "         [ 5632., 16896.],\n",
       "         [ 5632., 17408.],\n",
       "         [ 6144.,  2048.],\n",
       "         [ 6144.,  2560.],\n",
       "         [ 6144.,  3072.],\n",
       "         [ 6144.,  3584.],\n",
       "         [ 6144.,  6656.],\n",
       "         [ 6144.,  7168.],\n",
       "         [ 6144.,  7680.],\n",
       "         [ 6144.,  9216.],\n",
       "         [ 6144.,  9728.],\n",
       "         [ 6144., 10240.],\n",
       "         [ 6144., 10752.],\n",
       "         [ 6144., 11264.],\n",
       "         [ 6144., 11776.],\n",
       "         [ 6144., 12288.],\n",
       "         [ 6144., 12800.],\n",
       "         [ 6144., 13824.],\n",
       "         [ 6144., 15360.],\n",
       "         [ 6144., 15872.],\n",
       "         [ 6144., 16384.],\n",
       "         [ 6144., 16896.],\n",
       "         [ 6656.,  6656.],\n",
       "         [ 6656.,  7168.],\n",
       "         [ 6656.,  7680.],\n",
       "         [ 6656.,  9216.],\n",
       "         [ 6656.,  9728.],\n",
       "         [ 6656., 10240.],\n",
       "         [ 6656., 10752.],\n",
       "         [ 6656., 11264.],\n",
       "         [ 6656., 11776.],\n",
       "         [ 6656., 12288.],\n",
       "         [ 6656., 12800.],\n",
       "         [ 6656., 13312.],\n",
       "         [ 6656., 13824.],\n",
       "         [ 7168.,  9728.],\n",
       "         [ 7168., 10240.],\n",
       "         [ 7168., 10752.],\n",
       "         [ 7168., 11264.],\n",
       "         [ 7168., 11776.],\n",
       "         [ 7168., 12288.],\n",
       "         [ 7168., 12800.],\n",
       "         [ 7168., 13312.],\n",
       "         [ 7168., 13824.],\n",
       "         [ 7168., 16896.],\n",
       "         [ 7168., 17408.],\n",
       "         [ 7680.,  6144.],\n",
       "         [ 7680.,  6656.],\n",
       "         [ 7680.,  9728.],\n",
       "         [ 7680., 10240.],\n",
       "         [ 7680., 10752.],\n",
       "         [ 7680., 11264.],\n",
       "         [ 7680., 11776.],\n",
       "         [ 7680., 12288.],\n",
       "         [ 7680., 12800.],\n",
       "         [ 7680., 13312.],\n",
       "         [ 7680., 13824.],\n",
       "         [ 7680., 14336.],\n",
       "         [ 7680., 14848.],\n",
       "         [ 7680., 15360.],\n",
       "         [ 7680., 16384.],\n",
       "         [ 7680., 16896.],\n",
       "         [ 7680., 17408.],\n",
       "         [ 8192.,  6144.],\n",
       "         [ 8192.,  6656.],\n",
       "         [ 8192.,  7168.],\n",
       "         [ 8192., 11776.],\n",
       "         [ 8192., 12288.],\n",
       "         [ 8192., 12800.],\n",
       "         [ 8192., 13312.],\n",
       "         [ 8192., 13824.],\n",
       "         [ 8192., 14336.],\n",
       "         [ 8192., 14848.],\n",
       "         [ 8192., 15360.],\n",
       "         [ 8192., 15872.],\n",
       "         [ 8192., 16896.],\n",
       "         [ 8192., 17408.],\n",
       "         [ 8704.,  6656.],\n",
       "         [ 8704.,  8704.],\n",
       "         [ 8704.,  9216.],\n",
       "         [ 8704.,  9728.],\n",
       "         [ 8704., 10240.],\n",
       "         [ 8704., 12800.],\n",
       "         [ 8704., 13312.],\n",
       "         [ 8704., 13824.],\n",
       "         [ 8704., 14336.],\n",
       "         [ 8704., 14848.],\n",
       "         [ 8704., 15360.],\n",
       "         [ 8704., 15872.],\n",
       "         [ 9216.,  8704.],\n",
       "         [ 9216.,  9216.],\n",
       "         [ 9216.,  9728.],\n",
       "         [ 9216., 10240.],\n",
       "         [ 9216., 10752.],\n",
       "         [ 9216., 11264.],\n",
       "         [ 9216., 12800.],\n",
       "         [ 9216., 13312.],\n",
       "         [ 9216., 13824.],\n",
       "         [ 9216., 14336.],\n",
       "         [ 9216., 14848.],\n",
       "         [ 9216., 15360.],\n",
       "         [ 9216., 15872.],\n",
       "         [ 9728.,  8704.],\n",
       "         [ 9728.,  9216.],\n",
       "         [ 9728.,  9728.],\n",
       "         [ 9728., 10240.],\n",
       "         [ 9728., 10752.],\n",
       "         [ 9728., 11264.],\n",
       "         [ 9728., 12288.],\n",
       "         [ 9728., 12800.],\n",
       "         [ 9728., 15872.],\n",
       "         [ 9728., 16384.],\n",
       "         [ 9728., 16896.],\n",
       "         [10240.,  8704.],\n",
       "         [10240.,  9216.],\n",
       "         [10240., 10240.],\n",
       "         [10240., 10752.],\n",
       "         [10240., 11264.],\n",
       "         [10240., 12288.],\n",
       "         [10240., 12800.],\n",
       "         [10240., 13312.],\n",
       "         [10240., 16384.],\n",
       "         [10240., 16896.],\n",
       "         [10240., 17408.],\n",
       "         [10752.,  4096.],\n",
       "         [10752.,  4608.],\n",
       "         [10752.,  5120.],\n",
       "         [10752.,  5632.],\n",
       "         [10752.,  6144.],\n",
       "         [10752.,  8704.],\n",
       "         [10752.,  9216.],\n",
       "         [10752., 12288.],\n",
       "         [10752., 12800.],\n",
       "         [10752., 13312.],\n",
       "         [10752., 13824.],\n",
       "         [10752., 14336.],\n",
       "         [10752., 14848.],\n",
       "         [10752., 15360.],\n",
       "         [11264.,  3584.],\n",
       "         [11264.,  4096.],\n",
       "         [11264.,  4608.],\n",
       "         [11264.,  5120.],\n",
       "         [11264.,  5632.],\n",
       "         [11264.,  6144.],\n",
       "         [11264., 12288.],\n",
       "         [11264., 12800.],\n",
       "         [11264., 13312.],\n",
       "         [11264., 13824.],\n",
       "         [11264., 14336.],\n",
       "         [11264., 14848.],\n",
       "         [11264., 15360.],\n",
       "         [11776.,  3072.],\n",
       "         [11776.,  3584.],\n",
       "         [11776.,  4096.],\n",
       "         [11776.,  4608.],\n",
       "         [11776.,  5120.],\n",
       "         [11776.,  5632.],\n",
       "         [11776.,  6144.],\n",
       "         [11776.,  6656.],\n",
       "         [11776.,  7168.],\n",
       "         [11776.,  7680.],\n",
       "         [11776.,  8192.],\n",
       "         [11776., 11776.],\n",
       "         [11776., 12288.],\n",
       "         [11776., 12800.],\n",
       "         [11776., 13312.],\n",
       "         [11776., 13824.],\n",
       "         [11776., 14336.],\n",
       "         [11776., 14848.],\n",
       "         [11776., 15360.],\n",
       "         [12288.,  3072.],\n",
       "         [12288.,  3584.],\n",
       "         [12288.,  4096.],\n",
       "         [12288.,  4608.],\n",
       "         [12288.,  5120.],\n",
       "         [12288.,  5632.],\n",
       "         [12288.,  6144.],\n",
       "         [12288.,  6656.],\n",
       "         [12288.,  8192.],\n",
       "         [12288., 11264.],\n",
       "         [12288., 11776.],\n",
       "         [12288., 12288.],\n",
       "         [12288., 12800.],\n",
       "         [12288., 13312.],\n",
       "         [12288., 13824.],\n",
       "         [12288., 14336.],\n",
       "         [12288., 14848.],\n",
       "         [12288., 15360.],\n",
       "         [12800.,  3072.],\n",
       "         [12800.,  3584.],\n",
       "         [12800.,  4096.],\n",
       "         [12800.,  4608.],\n",
       "         [12800.,  5120.],\n",
       "         [12800.,  5632.],\n",
       "         [12800.,  6144.],\n",
       "         [12800.,  6656.],\n",
       "         [12800.,  7168.],\n",
       "         [12800., 10752.],\n",
       "         [12800., 11264.],\n",
       "         [12800., 11776.],\n",
       "         [12800., 12288.],\n",
       "         [12800., 12800.],\n",
       "         [12800., 13312.],\n",
       "         [12800., 13824.],\n",
       "         [12800., 14336.],\n",
       "         [12800., 14848.],\n",
       "         [12800., 15360.],\n",
       "         [13312.,  3072.],\n",
       "         [13312.,  3584.],\n",
       "         [13312.,  4096.],\n",
       "         [13312.,  4608.],\n",
       "         [13312.,  5120.],\n",
       "         [13312.,  5632.],\n",
       "         [13312.,  6144.],\n",
       "         [13312.,  6656.],\n",
       "         [13312.,  7168.],\n",
       "         [13312.,  9216.],\n",
       "         [13312.,  9728.],\n",
       "         [13312., 10752.],\n",
       "         [13312., 11264.],\n",
       "         [13312., 11776.],\n",
       "         [13312., 12288.],\n",
       "         [13312., 12800.],\n",
       "         [13824.,  3584.],\n",
       "         [13824.,  4096.],\n",
       "         [13824.,  4608.],\n",
       "         [13824.,  5120.],\n",
       "         [13824.,  5632.],\n",
       "         [13824.,  6144.],\n",
       "         [13824.,  6656.],\n",
       "         [13824.,  7168.],\n",
       "         [13824.,  8704.],\n",
       "         [13824.,  9216.],\n",
       "         [13824.,  9728.],\n",
       "         [13824., 10240.],\n",
       "         [13824., 11264.],\n",
       "         [13824., 11776.],\n",
       "         [13824., 12288.],\n",
       "         [13824., 13824.],\n",
       "         [13824., 14336.],\n",
       "         [13824., 14848.],\n",
       "         [14336.,  4608.],\n",
       "         [14336.,  5120.],\n",
       "         [14336.,  5632.],\n",
       "         [14336.,  6144.],\n",
       "         [14336.,  6656.],\n",
       "         [14336.,  8704.],\n",
       "         [14336.,  9216.],\n",
       "         [14336.,  9728.],\n",
       "         [14336., 10240.],\n",
       "         [14336., 11776.],\n",
       "         [14336., 12288.],\n",
       "         [14336., 12800.],\n",
       "         [14336., 13824.],\n",
       "         [14336., 14336.],\n",
       "         [14336., 14848.],\n",
       "         [14848.,  5120.],\n",
       "         [14848.,  5632.],\n",
       "         [14848.,  6144.],\n",
       "         [14848.,  6656.],\n",
       "         [14848.,  8704.],\n",
       "         [14848.,  9216.],\n",
       "         [14848.,  9728.],\n",
       "         [14848., 10240.],\n",
       "         [14848., 12288.],\n",
       "         [14848., 12800.],\n",
       "         [14848., 13312.],\n",
       "         [14848., 13824.],\n",
       "         [14848., 14336.],\n",
       "         [14848., 14848.],\n",
       "         [14848., 15360.],\n",
       "         [14848., 15872.],\n",
       "         [14848., 16384.],\n",
       "         [15360.,  5120.],\n",
       "         [15360.,  5632.],\n",
       "         [15360.,  6144.],\n",
       "         [15360.,  7680.],\n",
       "         [15360.,  8704.],\n",
       "         [15360.,  9216.],\n",
       "         [15360.,  9728.],\n",
       "         [15360., 10240.],\n",
       "         [15360., 13824.],\n",
       "         [15360., 14336.],\n",
       "         [15360., 14848.],\n",
       "         [15360., 15360.],\n",
       "         [15360., 15872.],\n",
       "         [15360., 16384.],\n",
       "         [15360., 17408.],\n",
       "         [15360., 17920.],\n",
       "         [15872.,  5120.],\n",
       "         [15872.,  7680.],\n",
       "         [15872.,  8192.],\n",
       "         [15872.,  9728.],\n",
       "         [15872., 10240.],\n",
       "         [15872., 14848.],\n",
       "         [15872., 15360.],\n",
       "         [15872., 15872.],\n",
       "         [15872., 16384.],\n",
       "         [15872., 17408.],\n",
       "         [15872., 17920.],\n",
       "         [16384.,  8704.],\n",
       "         [16384.,  9216.],\n",
       "         [16384.,  9728.],\n",
       "         [16896.,  9216.],\n",
       "         [16896.,  9728.],\n",
       "         [16896., 16384.],\n",
       "         [16896., 16896.],\n",
       "         [17408., 15360.],\n",
       "         [17408., 15872.],\n",
       "         [17408., 16384.],\n",
       "         [17408., 16896.],\n",
       "         [17920., 15872.]]),\n",
       " [0,\n",
       "  217,\n",
       "  241,\n",
       "  249,\n",
       "  232,\n",
       "  167,\n",
       "  376,\n",
       "  304,\n",
       "  307,\n",
       "  15,\n",
       "  116,\n",
       "  23,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af189cce-2505-4f09-bf13-1eac1dd9e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # super(CaptionModel, self).__init__()\n",
    "\n",
    "    # implements beam search\n",
    "    # calls beam_step and returns the final set of beams\n",
    "    # augments log-probabilities with diversity terms when number of groups > 1\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        mode = kwargs.get('mode', 'forward')\n",
    "        if 'mode' in kwargs:\n",
    "            del kwargs['mode']\n",
    "        return getattr(self, '_' + mode)(*args, **kwargs)\n",
    "\n",
    "    def beam_search(self, init_state, init_logprobs, *args, **kwargs):\n",
    "\n",
    "        # function computes the similarity score to be augmented\n",
    "        def add_diversity(beam_seq_table, logprobs, t, divm, diversity_lambda, bdash):\n",
    "            local_time = t - divm\n",
    "            unaug_logprobs = logprobs.clone()\n",
    "            batch_size = beam_seq_table[0].shape[0]\n",
    "\n",
    "            if divm > 0:\n",
    "                change = logprobs.new_zeros(batch_size, logprobs.shape[-1])\n",
    "                for prev_choice in range(divm):\n",
    "                    prev_decisions = beam_seq_table[prev_choice][:, :, local_time]  # Nxb\n",
    "                    for prev_labels in range(bdash):\n",
    "                        change.scatter_add_(1, prev_decisions[:, prev_labels].unsqueeze(-1),\n",
    "                                            change.new_ones(batch_size, 1))\n",
    "\n",
    "                if local_time == 0:\n",
    "                    logprobs = logprobs - change * diversity_lambda\n",
    "                else:\n",
    "                    logprobs = logprobs - repeat_tensors(bdash, change) * diversity_lambda\n",
    "\n",
    "            return logprobs, unaug_logprobs\n",
    "\n",
    "        # does one step of classical beam search\n",
    "\n",
    "        def beam_step(logprobs, unaug_logprobs, beam_size, t, beam_seq, beam_seq_logprobs, beam_logprobs_sum, state):\n",
    "            # INPUTS:\n",
    "            # logprobs: probabilities augmented after diversity N*bxV\n",
    "            # beam_size: obvious\n",
    "            # t        : time instant\n",
    "            # beam_seq : tensor contanining the beams\n",
    "            # beam_seq_logprobs: tensor contanining the beam logprobs\n",
    "            # beam_logprobs_sum: tensor contanining joint logprobs\n",
    "            # OUPUTS:\n",
    "            # beam_seq : tensor containing the word indices of the decoded captions Nxbxl\n",
    "            # beam_seq_logprobs : log-probability of each decision made, NxbxlxV\n",
    "            # beam_logprobs_sum : joint log-probability of each beam Nxb\n",
    "\n",
    "            batch_size = beam_logprobs_sum.shape[0]\n",
    "            vocab_size = logprobs.shape[-1]\n",
    "            logprobs = logprobs.reshape(batch_size, -1, vocab_size)  # NxbxV\n",
    "            if t == 0:\n",
    "                assert logprobs.shape[1] == 1\n",
    "                beam_logprobs_sum = beam_logprobs_sum[:, :1]\n",
    "            candidate_logprobs = beam_logprobs_sum.unsqueeze(-1) + logprobs  # beam_logprobs_sum Nxb logprobs is NxbxV\n",
    "            ys, ix = torch.sort(candidate_logprobs.reshape(candidate_logprobs.shape[0], -1), -1, True)\n",
    "            ys, ix = ys[:, :beam_size], ix[:, :beam_size]\n",
    "            beam_ix = ix // vocab_size  # Nxb which beam\n",
    "            selected_ix = ix % vocab_size  # Nxb # which world\n",
    "            state_ix = (beam_ix + torch.arange(batch_size).type_as(beam_ix).unsqueeze(-1) * logprobs.shape[1]).reshape(\n",
    "                -1)  # N*b which in Nxb beams\n",
    "\n",
    "            if t > 0:\n",
    "                # gather according to beam_ix\n",
    "                assert (beam_seq.gather(1, beam_ix.unsqueeze(-1).expand_as(beam_seq)) ==\n",
    "                        beam_seq.reshape(-1, beam_seq.shape[-1])[state_ix].view_as(beam_seq)).all()\n",
    "                beam_seq = beam_seq.gather(1, beam_ix.unsqueeze(-1).expand_as(beam_seq))\n",
    "\n",
    "                beam_seq_logprobs = beam_seq_logprobs.gather(1, beam_ix.unsqueeze(-1).unsqueeze(-1).expand_as(\n",
    "                    beam_seq_logprobs))\n",
    "\n",
    "            beam_seq = torch.cat([beam_seq, selected_ix.unsqueeze(-1)], -1)  # beam_seq Nxbxl\n",
    "            beam_logprobs_sum = beam_logprobs_sum.gather(1, beam_ix) + \\\n",
    "                                logprobs.reshape(batch_size, -1).gather(1, ix)\n",
    "            assert (beam_logprobs_sum == ys).all()\n",
    "            _tmp_beam_logprobs = unaug_logprobs[state_ix].reshape(batch_size, -1, vocab_size)\n",
    "            beam_logprobs = unaug_logprobs.reshape(batch_size, -1, vocab_size).gather(1,\n",
    "                                                                                      beam_ix.unsqueeze(-1).expand(-1,\n",
    "                                                                                                                   -1,\n",
    "                                                                                                                   vocab_size))  # NxbxV\n",
    "            assert (_tmp_beam_logprobs == beam_logprobs).all()\n",
    "            beam_seq_logprobs = torch.cat([\n",
    "                beam_seq_logprobs,\n",
    "                beam_logprobs.reshape(batch_size, -1, 1, vocab_size)], 2)\n",
    "\n",
    "            new_state = [None for _ in state]\n",
    "            for _ix in range(len(new_state)):\n",
    "                #  copy over state in previous beam q to new beam at vix\n",
    "                new_state[_ix] = state[_ix][:, state_ix]\n",
    "            state = new_state\n",
    "            return beam_seq, beam_seq_logprobs, beam_logprobs_sum, state\n",
    "\n",
    "        # Start diverse_beam_search\n",
    "        opt = kwargs['opt']\n",
    "        temperature = opt.get('temperature', 1)  # This should not affect beam search, but will affect dbs\n",
    "        beam_size = opt.get('beam_size', 10)\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        diversity_lambda = opt.get('diversity_lambda', 0.5)\n",
    "        decoding_constraint = opt.get('decoding_constraint', 0)\n",
    "        suppress_UNK = opt.get('suppress_UNK', 0)\n",
    "        length_penalty = penalty_builder(opt.get('length_penalty', ''))\n",
    "        bdash = beam_size // group_size  # beam per group\n",
    "\n",
    "        batch_size = init_logprobs.shape[0]\n",
    "        device = init_logprobs.device\n",
    "        # INITIALIZATIONS\n",
    "        beam_seq_table = [torch.LongTensor(batch_size, bdash, 0).to(device) for _ in range(group_size)]\n",
    "        beam_seq_logprobs_table = [torch.FloatTensor(batch_size, bdash, 0, self.vocab_size + 1).to(device) for _ in\n",
    "                                   range(group_size)]\n",
    "        beam_logprobs_sum_table = [torch.zeros(batch_size, bdash).to(device) for _ in range(group_size)]\n",
    "\n",
    "        # logprobs # logprobs predicted in last time step, shape (beam_size, vocab_size+1)\n",
    "        done_beams_table = [[[] for __ in range(group_size)] for _ in range(batch_size)]\n",
    "        state_table = [[_.clone() for _ in init_state] for _ in range(group_size)]\n",
    "        logprobs_table = [init_logprobs.clone() for _ in range(group_size)]\n",
    "        # END INIT\n",
    "\n",
    "        # Chunk elements in the args\n",
    "        args = list(args)\n",
    "        args = split_tensors(group_size, args)  # For each arg, turn (Bbg)x... to (Bb)x(g)x...\n",
    "        if self.__class__.__name__ == 'AttEnsemble':\n",
    "            args = [[[args[j][i][k] for i in range(len(self.models))] for j in range(len(args))] for k in\n",
    "                    range(group_size)]  # group_name, arg_name, model_name\n",
    "        else:\n",
    "            args = [[args[i][j] for i in range(len(args))] for j in range(group_size)]\n",
    "\n",
    "        for t in range(self.max_seq_length + group_size - 1):\n",
    "            for divm in range(group_size):\n",
    "                if t >= divm and t <= self.max_seq_length + divm - 1:\n",
    "                    # add diversity\n",
    "                    logprobs = logprobs_table[divm]\n",
    "                    # suppress previous word\n",
    "                    if decoding_constraint and t - divm > 0:\n",
    "                        logprobs.scatter_(1, beam_seq_table[divm][:, :, t - divm - 1].reshape(-1, 1).to(device),\n",
    "                                          float('-inf'))\n",
    "                    # suppress UNK tokens in the decoding\n",
    "                    if suppress_UNK:\n",
    "                        idx_unk = self.tokenizer.get_id_by_token('<unk>')\n",
    "                        logprobs[:, idx_unk] = logprobs[:, idx_unk] - 1000\n",
    "                        # diversity is added here\n",
    "                    # the function directly modifies the logprobs values and hence, we need to return\n",
    "                    # the unaugmented ones for sorting the candidates in the end. # for historical\n",
    "                    # reasons :-)\n",
    "                    logprobs, unaug_logprobs = add_diversity(beam_seq_table, logprobs, t, divm, diversity_lambda, bdash)\n",
    "\n",
    "                    # infer new beams\n",
    "                    beam_seq_table[divm], \\\n",
    "                    beam_seq_logprobs_table[divm], \\\n",
    "                    beam_logprobs_sum_table[divm], \\\n",
    "                    state_table[divm] = beam_step(logprobs,\n",
    "                                                  unaug_logprobs,\n",
    "                                                  bdash,\n",
    "                                                  t - divm,\n",
    "                                                  beam_seq_table[divm],\n",
    "                                                  beam_seq_logprobs_table[divm],\n",
    "                                                  beam_logprobs_sum_table[divm],\n",
    "                                                  state_table[divm])\n",
    "\n",
    "                    # log_message(f' t: {t}, divm: {divm}, beam_seq_table[divm]: {beam_seq_table[divm]}, beam_logprobs_sum_table: {beam_logprobs_sum_table}')\n",
    "                    # if time's up... or if end token is reached then copy beams\n",
    "                    for b in range(batch_size):\n",
    "                        is_end = beam_seq_table[divm][b, :, t - divm] == self.eos_idx\n",
    "                        assert beam_seq_table[divm].shape[-1] == t - divm + 1\n",
    "                        if t == self.max_seq_length + divm - 1:\n",
    "                            is_end.fill_(1)\n",
    "                        for vix in range(bdash):\n",
    "                            if is_end[vix]:\n",
    "                                final_beam = {\n",
    "                                    'seq': beam_seq_table[divm][b, vix].clone(),\n",
    "                                    'logps': beam_seq_logprobs_table[divm][b, vix].clone(),\n",
    "                                    'unaug_p': beam_seq_logprobs_table[divm][b, vix].sum().item(),\n",
    "                                    'p': beam_logprobs_sum_table[divm][b, vix].item()\n",
    "                                }\n",
    "                                log_message(f\"final_beam : {final_beam['seq']}, {final_beam['p']}, penalty: {length_penalty(t - divm + 1, final_beam['p'])}\")\n",
    "                                final_beam['p'] = length_penalty(t - divm + 1, final_beam['p'])\n",
    "                                done_beams_table[b][divm].append(final_beam)\n",
    "                        beam_logprobs_sum_table[divm][b, is_end] -= 1000\n",
    "                        \n",
    "                    # log_message(f\"done_beams_table: {list(map(lambda x: (x['seq'], x['p']), done_beams_table[0][0]))}\")\n",
    "                    # move the current group one step forward in time\n",
    "\n",
    "                    it = beam_seq_table[divm][:, :, t - divm].reshape(-1)\n",
    "                    logprobs_table[divm], state_table[divm] = self.get_logprobs_state(it.cuda(), *(\n",
    "                            args[divm] + [state_table[divm]]))\n",
    "                    logprobs_table[divm] = F.log_softmax(logprobs_table[divm] / temperature, dim=-1)\n",
    "\n",
    "        # all beams are sorted by their log-probabilities\n",
    "        done_beams_table = [[sorted(done_beams_table[b][i], key=lambda x: -x['p'])[:bdash] for i in range(group_size)]\n",
    "                            for b in range(batch_size)]\n",
    "        done_beams = [sum(_, []) for _ in done_beams_table]\n",
    "\n",
    "        # print(f'done_beams: {done_beams}')\n",
    "        log_message(f\"done_beams: {list(map(lambda x: (x['seq'], x['p']), done_beams[0]))}\")\n",
    "        return done_beams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "537bcc07-2db9-4bd7-8a0f-b1aad873f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention model\n",
    "class AttModel(CaptionModel):\n",
    "    def __init__(self, args, tokenizer):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.input_encoding_size = args.d_model\n",
    "        self.rnn_size = args.d_ff\n",
    "        self.num_layers = args.num_layers\n",
    "        self.drop_prob_lm = args.drop_prob_lm\n",
    "        self.max_seq_length = args.max_seq_length\n",
    "        self.att_feat_size = args.d_vf\n",
    "        self.att_hid_size = args.d_model\n",
    "\n",
    "        self.bos_idx = args.bos_idx\n",
    "        self.eos_idx = args.eos_idx\n",
    "        self.pad_idx = args.pad_idx\n",
    "\n",
    "        self.use_bn = args.use_bn\n",
    "\n",
    "        self.embed = lambda x: x\n",
    "        self.fc_embed = lambda x: x\n",
    "        self.att_embed = nn.Sequential(*(\n",
    "                ((nn.BatchNorm1d(self.att_feat_size),) if self.use_bn else ()) +\n",
    "                (nn.Linear(self.att_feat_size, self.input_encoding_size),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Dropout(self.drop_prob_lm)) +\n",
    "                ((nn.BatchNorm1d(self.input_encoding_size),) if self.use_bn == 2 else ())))\n",
    "\n",
    "        self.out1 = nn.Sequential(\n",
    "            nn.Linear(self.att_feat_size, 1024),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.out2 = nn.Sequential(\n",
    "            nn.Linear(self.rnn_size, self.rnn_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(self.att_feat_size)\n",
    "\n",
    "    def clip_att(self, att_feats, att_masks):\n",
    "        # Clip the length of att_masks and att_feats to the maximum length\n",
    "        if att_masks is not None:\n",
    "            max_len = att_masks.data.long().sum(1).max()\n",
    "            att_feats = att_feats[:, :max_len].contiguous()\n",
    "            att_masks = att_masks[:, :max_len].contiguous()\n",
    "        return att_feats, att_masks\n",
    "\n",
    "    def multimodal_feat(self, att_feats, meshes):# Concate multimodal features\n",
    "        return torch.cat((self.ln(att_feats),self.ln(meshes)),dim=1)\n",
    "        # return torch.cat((self.ln(self.out1(att_feats)),self.ln(self.out2(meshes))),dim=1)\n",
    "\n",
    "    def _prepare_feature(self, fc_feats, att_feats, att_masks):\n",
    "        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n",
    "\n",
    "        # embed fc and att feats\n",
    "        fc_feats = self.fc_embed(fc_feats)\n",
    "        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n",
    "\n",
    "        # Project the attention feats first to reduce memory and computation comsumptions.\n",
    "        p_att_feats = self.ctx2att(att_feats)\n",
    "\n",
    "        return fc_feats, att_feats, p_att_feats, att_masks\n",
    "\n",
    "    def get_logprobs_state(self, it, fc_feats, att_feats, p_att_feats, att_masks, state, output_logsoftmax=1):\n",
    "        # 'it' contains a word index\n",
    "        xt = self.embed(it)\n",
    "\n",
    "        output, state = self.core(xt, fc_feats, att_feats, p_att_feats, state, att_masks)\n",
    "        if output_logsoftmax:\n",
    "            logprobs = F.log_softmax(self.logit(output), dim=1)\n",
    "        else:\n",
    "            logprobs = self.logit(output)\n",
    "\n",
    "        return logprobs, state\n",
    "\n",
    "    def _sample_beam(self, fc_feats, att_feats, att_masks=None, meshes=None, opt={}):\n",
    "        # print(f'opt: {opt}')\n",
    "        beam_size = opt.get('beam_size', 10)\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        sample_n = opt.get('sample_n', 10)\n",
    "        # when sample_n == beam_size then each beam is a sample.\n",
    "        assert sample_n == 1 or sample_n == beam_size // group_size, 'when beam search, sample_n == 1 or beam search'\n",
    "        batch_size = fc_feats.size(0)\n",
    "\n",
    "        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks)\n",
    "\n",
    "        assert beam_size <= self.vocab_size + 1, 'lets assume this for now, otherwise this corner case causes a few headaches down the road. can be dealt with in future if needed'\n",
    "        seq = fc_feats.new_full((batch_size * sample_n, self.max_seq_length), self.pad_idx, dtype=torch.long)\n",
    "        seqLogprobs = fc_feats.new_zeros(batch_size * sample_n, self.max_seq_length, self.vocab_size + 1)\n",
    "        # lets process every image independently for now, for simplicity\n",
    "\n",
    "        self.done_beams = [[] for _ in range(batch_size)]\n",
    "\n",
    "        state = self.init_hidden(batch_size)\n",
    "\n",
    "        # first step, feed bos\n",
    "        it = fc_feats.new_full([batch_size], self.bos_idx, dtype=torch.long)\n",
    "        logprobs, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, state)\n",
    "\n",
    "        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = repeat_tensors(beam_size,\n",
    "                                                                                  [p_fc_feats, p_att_feats,\n",
    "                                                                                   pp_att_feats, p_att_masks]\n",
    "                                                                                  )\n",
    "        self.done_beams = self.beam_search(state, logprobs, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, opt=opt)\n",
    "        # print(f'self.done_beams: {self.done_beams}')\n",
    "        for k in range(batch_size):\n",
    "            if sample_n == beam_size:\n",
    "                for _n in range(sample_n):\n",
    "                    seq_len = self.done_beams[k][_n]['seq'].shape[0]\n",
    "                    seq[k * sample_n + _n, :seq_len] = self.done_beams[k][_n]['seq']\n",
    "                    seqLogprobs[k * sample_n + _n, :seq_len] = self.done_beams[k][_n]['logps']\n",
    "                    # print(f'---> seq: {seq}, seqLogprobs: {seqLogprobs}')\n",
    "            else:\n",
    "                seq_len = self.done_beams[k][0]['seq'].shape[0]\n",
    "                seq[k, :seq_len] = self.done_beams[k][0]['seq']  # the first beam has highest cumulative score\n",
    "                seqLogprobs[k, :seq_len] = self.done_beams[k][0]['logps']\n",
    "        # return the samples and their log likelihoods\n",
    "        # print(f'_sample_beam: seq {seq}, seqLogprobs {seqLogprobs}')\n",
    "        return seq, seqLogprobs\n",
    "\n",
    "    def _sample(self, fc_feats, att_feats, meshes=None, att_masks=None):\n",
    "        opt = self.args.__dict__\n",
    "        sample_method = opt.get('sample_method', 'greedy')\n",
    "        beam_size = opt.get('beam_size', 1)\n",
    "        temperature = opt.get('temperature', 1.0)\n",
    "        sample_n = int(opt.get('sample_n', 1))\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        output_logsoftmax = opt.get('output_logsoftmax', 1)\n",
    "        decoding_constraint = opt.get('decoding_constraint', 0)\n",
    "        block_trigrams = opt.get('block_trigrams', 0)\n",
    "        if beam_size > 1 and sample_method in ['greedy', 'beam_search']:\n",
    "            return self._sample_beam(fc_feats, att_feats, att_masks, meshes, opt)\n",
    "        if group_size > 1:\n",
    "            return self._diverse_sample(fc_feats, att_feats, att_masks, meshes, opt)\n",
    "\n",
    "        batch_size = fc_feats.size(0)\n",
    "        state = self.init_hidden(batch_size * sample_n)\n",
    "\n",
    "        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks, meshes)\n",
    "\n",
    "        if sample_n > 1:\n",
    "            p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = repeat_tensors(sample_n,\n",
    "                                                                                      [p_fc_feats, p_att_feats,\n",
    "                                                                                       pp_att_feats, p_att_masks]\n",
    "                                                                                      )\n",
    "\n",
    "        trigrams = []  # will be a list of batch_size dictionaries\n",
    "\n",
    "        seq = fc_feats.new_full((batch_size * sample_n, self.max_seq_length), self.pad_idx, dtype=torch.long)\n",
    "        seqLogprobs = fc_feats.new_zeros(batch_size * sample_n, self.max_seq_length, self.vocab_size + 1)\n",
    "        for t in range(self.max_seq_length + 1):\n",
    "            if t == 0:  # input <bos>\n",
    "                it = fc_feats.new_full([batch_size * sample_n], self.bos_idx, dtype=torch.long)\n",
    "\n",
    "            logprobs, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, state,\n",
    "                                                      output_logsoftmax=output_logsoftmax)\n",
    "\n",
    "            if decoding_constraint and t > 0:\n",
    "                tmp = logprobs.new_zeros(logprobs.size())\n",
    "                tmp.scatter_(1, seq[:, t - 1].data.unsqueeze(1), float('-inf'))\n",
    "                logprobs = logprobs + tmp\n",
    "\n",
    "            # Mess with trigrams\n",
    "            # Copy from https://github.com/lukemelas/image-paragraph-captioning\n",
    "            if block_trigrams and t >= 3:\n",
    "                # Store trigram generated at last step\n",
    "                prev_two_batch = seq[:, t - 3:t - 1]\n",
    "                for i in range(batch_size):  # = seq.size(0)\n",
    "                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                    current = seq[i][t - 1]\n",
    "                    if t == 3:  # initialize\n",
    "                        trigrams.append({prev_two: [current]})  # {LongTensor: list containing 1 int}\n",
    "                    elif t > 3:\n",
    "                        if prev_two in trigrams[i]:  # add to list\n",
    "                            trigrams[i][prev_two].append(current)\n",
    "                        else:  # create list\n",
    "                            trigrams[i][prev_two] = [current]\n",
    "                # Block used trigrams at next step\n",
    "                prev_two_batch = seq[:, t - 2:t]\n",
    "                mask = torch.zeros(logprobs.size(), requires_grad=False).cuda()  # batch_size x vocab_size\n",
    "                for i in range(batch_size):\n",
    "                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                    if prev_two in trigrams[i]:\n",
    "                        for j in trigrams[i][prev_two]:\n",
    "                            mask[i, j] += 1\n",
    "                # Apply mask to log probs\n",
    "                # logprobs = logprobs - (mask * 1e9)\n",
    "                alpha = 2.0  # = 4\n",
    "                logprobs = logprobs + (mask * -0.693 * alpha)  # ln(1/2) * alpha (alpha -> infty works best)\n",
    "\n",
    "            # sample the next word\n",
    "            if t == self.max_seq_length:  # skip if we achieve maximum length\n",
    "                break\n",
    "            it, sampleLogprobs = self.sample_next_word(logprobs, sample_method, temperature)\n",
    "\n",
    "            # stop when all finished\n",
    "            if t == 0:\n",
    "                unfinished = it != self.eos_idx\n",
    "            else:\n",
    "                it[~unfinished] = self.pad_idx  # This allows eos_idx not being overwritten to 0\n",
    "                logprobs = logprobs * unfinished.unsqueeze(1).float()\n",
    "                unfinished = unfinished * (it != self.eos_idx)\n",
    "            seq[:, t] = it\n",
    "            seqLogprobs[:, t] = logprobs\n",
    "            # quit loop if all sequences have finished\n",
    "            if unfinished.sum() == 0:\n",
    "                break\n",
    "\n",
    "        return seq, seqLogprobs\n",
    "\n",
    "    def _diverse_sample(self, fc_feats, att_feats, att_masks=None, opt={}):\n",
    "\n",
    "        sample_method = opt.get('sample_method', 'greedy')\n",
    "        beam_size = opt.get('beam_size', 1)\n",
    "        temperature = opt.get('temperature', 1.0)\n",
    "        group_size = opt.get('group_size', 1)\n",
    "        diversity_lambda = opt.get('diversity_lambda', 0.5)\n",
    "        decoding_constraint = opt.get('decoding_constraint', 0)\n",
    "        block_trigrams = opt.get('block_trigrams', 0)\n",
    "\n",
    "        batch_size = fc_feats.size(0)\n",
    "        state = self.init_hidden(batch_size)\n",
    "\n",
    "        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks)\n",
    "\n",
    "        trigrams_table = [[] for _ in range(group_size)]  # will be a list of batch_size dictionaries\n",
    "\n",
    "        seq_table = [fc_feats.new_full((batch_size, self.max_seq_length), self.pad_idx, dtype=torch.long) for _ in\n",
    "                     range(group_size)]\n",
    "        seqLogprobs_table = [fc_feats.new_zeros(batch_size, self.max_seq_length) for _ in range(group_size)]\n",
    "        state_table = [self.init_hidden(batch_size) for _ in range(group_size)]\n",
    "\n",
    "        for tt in range(self.max_seq_length + group_size):\n",
    "            for divm in range(group_size):\n",
    "                t = tt - divm\n",
    "                seq = seq_table[divm]\n",
    "                seqLogprobs = seqLogprobs_table[divm]\n",
    "                trigrams = trigrams_table[divm]\n",
    "                if t >= 0 and t <= self.max_seq_length - 1:\n",
    "                    if t == 0:  # input <bos>\n",
    "                        it = fc_feats.new_full([batch_size], self.bos_idx, dtype=torch.long)\n",
    "                    else:\n",
    "                        it = seq[:, t - 1]  # changed\n",
    "\n",
    "                    logprobs, state_table[divm] = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats,\n",
    "                                                                          p_att_masks, state_table[divm])  # changed\n",
    "                    logprobs = F.log_softmax(logprobs / temperature, dim=-1)\n",
    "\n",
    "                    # Add diversity\n",
    "                    if divm > 0:\n",
    "                        unaug_logprobs = logprobs.clone()\n",
    "                        for prev_choice in range(divm):\n",
    "                            prev_decisions = seq_table[prev_choice][:, t]\n",
    "                            logprobs[:, prev_decisions] = logprobs[:, prev_decisions] - diversity_lambda\n",
    "\n",
    "                    if decoding_constraint and t > 0:\n",
    "                        tmp = logprobs.new_zeros(logprobs.size())\n",
    "                        tmp.scatter_(1, seq[:, t - 1].data.unsqueeze(1), float('-inf'))\n",
    "                        logprobs = logprobs + tmp\n",
    "\n",
    "                    # Mess with trigrams\n",
    "                    if block_trigrams and t >= 3:\n",
    "                        # Store trigram generated at last step\n",
    "                        prev_two_batch = seq[:, t - 3:t - 1]\n",
    "                        for i in range(batch_size):  # = seq.size(0)\n",
    "                            prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                            current = seq[i][t - 1]\n",
    "                            if t == 3:  # initialize\n",
    "                                trigrams.append({prev_two: [current]})  # {LongTensor: list containing 1 int}\n",
    "                            elif t > 3:\n",
    "                                if prev_two in trigrams[i]:  # add to list\n",
    "                                    trigrams[i][prev_two].append(current)\n",
    "                                else:  # create list\n",
    "                                    trigrams[i][prev_two] = [current]\n",
    "                        # Block used trigrams at next step\n",
    "                        prev_two_batch = seq[:, t - 2:t]\n",
    "                        mask = torch.zeros(logprobs.size(), requires_grad=False).cuda()  # batch_size x vocab_size\n",
    "                        for i in range(batch_size):\n",
    "                            prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n",
    "                            if prev_two in trigrams[i]:\n",
    "                                for j in trigrams[i][prev_two]:\n",
    "                                    mask[i, j] += 1\n",
    "                        # Apply mask to log probs\n",
    "                        # logprobs = logprobs - (mask * 1e9)\n",
    "                        alpha = 2.0  # = 4\n",
    "                        logprobs = logprobs + (mask * -0.693 * alpha)  # ln(1/2) * alpha (alpha -> infty works best)\n",
    "\n",
    "                    it, sampleLogprobs = self.sample_next_word(logprobs, sample_method, 1)\n",
    "\n",
    "                    # stop when all finished\n",
    "                    if t == 0:\n",
    "                        unfinished = it != self.eos_idx\n",
    "                    else:\n",
    "                        unfinished = seq[:, t - 1] != self.pad_idx & seq[:, t - 1] != self.eos_idx\n",
    "                        it[~unfinished] = self.pad_idx\n",
    "                        unfinished = unfinished & (it != self.eos_idx)  # changed\n",
    "                    seq[:, t] = it\n",
    "                    seqLogprobs[:, t] = sampleLogprobs.view(-1)\n",
    "\n",
    "        return torch.stack(seq_table, 1).reshape(batch_size * group_size, -1), torch.stack(seqLogprobs_table,\n",
    "                                                                                           1).reshape(\n",
    "            batch_size * group_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e1ecac-b461-46e8-ac58-2ea31ffe5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbfaaa8-391a-452d-b1a1-fac5617ecd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N, PAM):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.d_model)\n",
    "        self.PAM = clones(PAM, N)\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        s=[]\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            x = layer(x, mask)\n",
    "            s.append(self.PAM[i](x))\n",
    "\n",
    "\n",
    "        o = s[0]\n",
    "        for i in range(1,len(s)):\n",
    "            o +=  s[i]\n",
    "        return o\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(d_model, dropout), 2)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0979b08d-c47a-4564-8014-b63cc956f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, self_attn, src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(d_model, dropout), 3)\n",
    "\n",
    "    def forward(self, x, hidden_states, src_mask, tgt_mask):\n",
    "        m = hidden_states\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.d_model)\n",
    "\n",
    "    def forward(self, x, hidden_states, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, hidden_states, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3c7a32-296c-4aa0-9821-064599c87fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, hidden_states, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), hidden_states, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        x, self.attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class PAM(nn.Module):\n",
    "    def __init__(self, dim=512):\n",
    "        super(PAM, self).__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim, 13, 1, 13//2, groups=dim)\n",
    "        self.proj1 = nn.Conv2d(dim, dim, 7, 1, 7//2, groups=dim)\n",
    "        self.proj2 = nn.Conv2d(dim, dim, 3, 1, 3//2, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, C = x.shape\n",
    "        assert int(math.sqrt(H))**2==H, f'{x.shape}'\n",
    "        cnn_feat = x.transpose(1, 2).view(B, C, int(math.sqrt(H)), int(math.sqrt(H))).contiguous()\n",
    "        x = self.proj(cnn_feat)+cnn_feat+self.proj1(cnn_feat)+self.proj2(cnn_feat)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderDecoder(AttModel):\n",
    "\n",
    "    def __init__(self, args, tokenizer):\n",
    "        super().__init__(args, tokenizer)\n",
    "        self.args = args\n",
    "        self.num_layers = args.num_layers\n",
    "        self.d_model = args.d_model\n",
    "        self.d_ff = args.d_ff\n",
    "        self.num_heads = args.num_heads\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        tgt_vocab = self.vocab_size + 1\n",
    "\n",
    "        self.embeded = Embeddings(args.d_vf, tgt_vocab)\n",
    "        self.model = self.__build_model(tgt_vocab)\n",
    "        self.__init_model()\n",
    "\n",
    "        self.logit = nn.Linear(args.d_model, tgt_vocab)\n",
    "        self.logit_mesh = nn.Linear(args.d_model, args.d_model)\n",
    "\n",
    "    def __build_model(self, tgt_vocab):\n",
    "        attn = MultiHeadedAttention(self.num_heads, self.d_model)\n",
    "        ff = PositionwiseFeedForward(self.d_model, self.d_ff, self.dropout)\n",
    "        position = PositionalEncoding(self.d_model, self.dropout)\n",
    "        pp = PAM(self.d_model)\n",
    "        model = Transformer(\n",
    "            Encoder(EncoderLayer(self.d_model, deepcopy(attn), deepcopy(ff), self.dropout), self.num_layers, pp),\n",
    "            Decoder(\n",
    "                DecoderLayer(self.d_model, deepcopy(attn), deepcopy(attn), deepcopy(ff), self.dropout),\n",
    "                self.num_layers),\n",
    "            lambda x: x,\n",
    "            nn.Sequential(Embeddings(self.d_model, tgt_vocab), deepcopy(position))\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        return []\n",
    "\n",
    "    def __init_model(self):\n",
    "        for p in self.model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def _prepare_feature(self, fc_feats, att_feats, att_masks, meshes=None):\n",
    "        att_feats = pad_tokens(att_feats)\n",
    "        att_feats, seq, _, att_masks, seq_mask, _ = self._prepare_feature_forward(att_feats, att_masks, meshes)\n",
    "        memory = self.model.encode(att_feats, att_masks)\n",
    "\n",
    "        return fc_feats[..., :1], att_feats[..., :1], memory, att_masks\n",
    "\n",
    "    def _prepare_feature_mesh(self, att_feats, att_masks=None, meshes=None):\n",
    "        att_feats = pad_tokens(att_feats)\n",
    "        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n",
    "        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n",
    "\n",
    "        if att_masks is None:\n",
    "            att_masks = att_feats.new_ones(att_feats.shape[:2], dtype=torch.long)\n",
    "        att_masks = att_masks.unsqueeze(-2)\n",
    "\n",
    "        if meshes is not None:\n",
    "            # crop the last one\n",
    "            meshes = meshes[:, :-1]\n",
    "            meshes_mask = (meshes.data > 0)\n",
    "            meshes_mask[:, 0] += True\n",
    "\n",
    "            meshes_mask = meshes_mask.unsqueeze(-2)\n",
    "            meshes_mask = meshes_mask & subsequent_mask(meshes.size(-1)).to(meshes_mask)\n",
    "        else:\n",
    "            meshes_mask = None\n",
    "\n",
    "        return att_feats, meshes, att_masks, meshes_mask\n",
    "\n",
    "    def _prepare_feature_forward(self, att_feats, att_masks=None, meshes=None, seq=None):\n",
    "\n",
    "        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n",
    "        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n",
    "\n",
    "        if att_masks is None:\n",
    "            att_masks = att_feats.new_ones(att_feats.shape[:2], dtype=torch.long)\n",
    "        att_masks = att_masks.unsqueeze(-2)\n",
    "\n",
    "        if seq is not None:\n",
    "            # crop the last one\n",
    "            seq = seq[:, :-1]\n",
    "            seq_mask = (seq.data > 0)\n",
    "            seq_mask[:, 0] += True\n",
    "\n",
    "            seq_mask = seq_mask.unsqueeze(-2)\n",
    "            seq_mask = seq_mask & subsequent_mask(seq.size(-1)).to(seq_mask)\n",
    "        else:\n",
    "            seq_mask = None\n",
    "\n",
    "        if meshes is not None:\n",
    "            # crop the last one\n",
    "            meshes = meshes[:, :-1]\n",
    "            meshes_mask = (meshes.data > 0)\n",
    "            meshes_mask[:, 0] += True\n",
    "\n",
    "            meshes_mask = meshes_mask.unsqueeze(-2)\n",
    "            meshes_mask = meshes_mask & subsequent_mask(meshes.size(-1)).to(meshes_mask)\n",
    "        else:\n",
    "            meshes_mask = None\n",
    "\n",
    "        return att_feats, seq, meshes, att_masks, seq_mask, meshes_mask\n",
    "\n",
    "    def _forward(self, fc_feats, att_feats, report_ids, att_masks=None):\n",
    "        # log_message(fc_feats, att_feats, report_ids, att_masks)\n",
    "        att_feats, report_ids, att_masks, report_mask = self._prepare_feature_mesh(att_feats, att_masks, report_ids)\n",
    "        out = self.model(att_feats, report_ids, att_masks, report_mask)\n",
    "\n",
    "        # print(f'out: {out}')\n",
    "        outputs = F.log_softmax(self.logit(out), dim=-1)\n",
    "        # print(f'outputs: {outputs}')\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def core(self, it, fc_feats_ph, att_feats_ph, memory, state, mask):\n",
    "\n",
    "        if len(state) == 0:\n",
    "            ys = it.long().unsqueeze(1)\n",
    "        else:\n",
    "            ys = torch.cat([state[0][0], it.unsqueeze(1)], dim=1)\n",
    "        out = self.model.decode(memory, mask, ys, subsequent_mask(ys.size(1)).to(memory.device))\n",
    "        return out[:, -1], [ys.unsqueeze(0)]\n",
    "\n",
    "    def _encode(self, fc_feats, att_feats, att_masks=None):\n",
    "\n",
    "        att_feats, _, att_masks, _ = self._prepare_feature_mesh(att_feats, att_masks)\n",
    "        out = self.model.encode(att_feats, att_masks)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1669a405-5836-4fc2-9b46-e78231e64126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReportGenModel(nn.Module):\n",
    "\n",
    "    def __init__(self, args, tokenizer):\n",
    "        super().__init__()\n",
    "        self.__tokenizer = tokenizer\n",
    "\n",
    "        self.prompt = nn.Parameter(torch.randn(1, 1, args.d_vf))\n",
    "\n",
    "\n",
    "        self.positional_encoder = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, args.d_vf)\n",
    "        )\n",
    "\n",
    "        self.encoder_decoder = EncoderDecoder(args, tokenizer)\n",
    "\n",
    "\n",
    "    def forward(self, image_embeddings, pos_embeddings, report_ids, patch_masks, mode='train'):\n",
    "        coords_encoded = self.positional_encoder(pos_embeddings)\n",
    "        patch_feats = image_embeddings + coords_encoded\n",
    "\n",
    "        att_feats = torch.cat([self.prompt, patch_feats], dim=1)\n",
    "        fc_feats = torch.sum(att_feats, dim=1)\n",
    "        if mode == 'train':\n",
    "            output = self.encoder_decoder(fc_feats, att_feats, report_ids, mode='forward')\n",
    "        elif mode == 'sample':\n",
    "            output, _ = self.encoder_decoder(fc_feats, att_feats, mode='sample')\n",
    "        elif mode == 'encode':\n",
    "            output = self.encoder_decoder(fc_feats, att_feats, mode='encode')\n",
    "\n",
    "            logits = self.fc(output[0,0,:]).unsqueeze(0)\n",
    "            Y_hat = torch.argmax(logits, dim=1)\n",
    "            Y_prob = F.softmax(logits, dim=1)\n",
    "            return Y_hat, Y_prob\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c952eb18-e13b-4640-ace8-10d3f91e0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        # truncate to the same size\n",
    "        target = target[:, :input.size(1)]\n",
    "        mask = mask[:, :input.size(1)]\n",
    "        output = -input.gather(2, target.long().unsqueeze(2)).squeeze(2) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a158325-a3f6-47e6-a049-fe0de679b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "class ReportModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, tokenizer, weight_decay=0.01):\n",
    "        super().__init__()\n",
    "        self.model = ReportGenModel(args, tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.__lr = args.lr\n",
    "        self.__weight_decay = weight_decay\n",
    "        self.rouge = ROUGEScore()\n",
    "        self.bleu = BLEUScore(n_gram=1)\n",
    "\n",
    "    def loss_fn(self, output, reports_ids, reports_masks):\n",
    "        \n",
    "        criterion = LanguageModelCriterion()\n",
    "        loss = criterion(output, reports_ids[:, 1:], reports_masks[:, 1:]).mean()\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        # print('train ---------->')\n",
    "        _, patch_feats, pos_feats, report_ids, report_masks, patch_masks = batch\n",
    "        output = self.model(patch_feats, pos_feats, report_ids, patch_masks, mode='train')\n",
    "        # print(f'train output: {output}')\n",
    "        loss = self.loss_fn(output, report_ids, report_masks)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # print('val ---------->')\n",
    "        _, patch_feats, pos_feats, report_ids, report_masks, patch_masks = batch\n",
    "        \n",
    "        output_ = self.model(patch_feats, pos_feats, report_ids, patch_masks, mode='train')\n",
    "        # print(f'val output: {output_}')\n",
    "        loss = self.loss_fn(output_, report_ids, report_masks)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        \n",
    "        if batch_idx%100==0:\n",
    "            output = self.model(patch_feats, pos_feats, report_ids,patch_masks, mode='sample')\n",
    "            pred_texts = self.tokenizer.batch_decode(output.cpu().numpy())\n",
    "            target_texts = self.tokenizer.batch_decode(report_ids[:, 1:].cpu().numpy())\n",
    "            # print(f'val output: {output_}')\n",
    "            # print(f'report_ids: {report_ids}, output: {output}')\n",
    "            print(f'pred_texts: {pred_texts}, target_texts: {target_texts}')\n",
    "\n",
    "            rouge_score = self.rouge(pred_texts, target_texts)\n",
    "            bleu_score = self.bleu(pred_texts, target_texts)\n",
    "    \n",
    "            self.log('val_rouge', rouge_score['rouge1_fmeasure'], on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "            self.log('val_bleu', bleu_score, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        d_params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = torch.optim.AdamW(d_params, lr=self.__lr, weight_decay=self.__weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08be2c94-434e-46f4-996b-9183f741ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datamodule\n",
    "\n",
    "class PatchEmbeddingDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,args, tokenizer, split_frac, shuffle = False):\n",
    "        super().__init__()\n",
    "        self.test_ds = None\n",
    "        self.val_ds = None\n",
    "        self.train_ds = None\n",
    "        self.__batch_size = args.batch_size\n",
    "        self.__shuffle = shuffle\n",
    "        self.__num_workers = args.num_workers\n",
    "        self.__embeddings_path = args.embeddings_path\n",
    "        self.__reports_json_path = args.reports_json_path\n",
    "        self.__max_seq_length = args.max_seq_length\n",
    "        self.__split_frac = split_frac\n",
    "        self.__tokenizer = tokenizer\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = EmbeddingDataset(self.__embeddings_path, self.__reports_json_path, self.__tokenizer,\n",
    "                              self.__max_seq_length)\n",
    "        self.train_ds, self.val_ds, self.test_ds = random_split(dataset, self.__split_frac)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.__batch_size, shuffle=self.__shuffle, collate_fn = self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.__batch_size, collate_fn = self.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.__batch_size, collate_fn = self.collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        slide_ids, patch_feats, coord_feats, report_ids, report_masks, seq_length = zip(*batch)\n",
    "        patch_feats_pad = pad_sequence(patch_feats, batch_first=True)\n",
    "        coord_feats_pad =  pad_sequence(coord_feats, batch_first=True)\n",
    "        patch_mask = torch.zeros(patch_feats_pad.shape[:2], dtype=torch.float32)\n",
    "        for i, p in enumerate(patch_feats):\n",
    "            patch_mask[i, :p.shape[0]] = 1\n",
    "\n",
    "        return (slide_ids, patch_feats_pad, coord_feats_pad, torch.LongTensor(report_ids),\n",
    "                torch.FloatTensor(report_masks), torch.FloatTensor(patch_mask))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb2e293-49ff-4cd2-a1fd-17a0881d78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, args, model, tokenizer, split_frac=(0.75, 0.12, 0.13)):\n",
    "        self.ckpt_path = args.ckpt_path\n",
    "        self.max_epochs = args.max_epochs\n",
    "        self.split_frac = split_frac\n",
    "        self.datamodule = PatchEmbeddingDataModule(args, tokenizer, split_frac)\n",
    "        self.model = model\n",
    "        pl.seed_everything(42)\n",
    "\n",
    "    def train(self, fast_dev_run=False):\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=self.ckpt_path,  # Directory to save checkpoints\n",
    "            filename=\"best_model\",  # Naming convention\n",
    "            monitor=\"val_loss\",  # Metric to monitor for saving best checkpoints\n",
    "            mode=\"min\",  # Whether to minimize or maximize the monitored metric\n",
    "            save_top_k=1,  # Number of best checkpoints to keep\n",
    "            save_last=True  # Save the last checkpoint regardless of the monitored metric\n",
    "        )\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=3, verbose=True, mode=\"min\")\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            accelerator='gpu',\n",
    "            devices=[1],\n",
    "            strategy='auto',\n",
    "            enable_progress_bar=True,\n",
    "            log_every_n_steps=2,\n",
    "            fast_dev_run=fast_dev_run\n",
    "        )\n",
    "        train_metrics = trainer.fit(\n",
    "            self.model, datamodule=self.datamodule\n",
    "        )\n",
    "\n",
    "        return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eb77f61-e3a7-4bcc-a07d-1277d7202145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'max_fea_length': 10000,\n",
    "    'max_seq_length': 50,\n",
    "    'threshold': 1,\n",
    "    'batch_size': 1,\n",
    "    'ckpt_path': 'checkpoints/1',\n",
    "    'max_epochs': 100,\n",
    "    'd_ff': 512,\n",
    "    'd_vf': 768,\n",
    "    'd_model': 512,\n",
    "    'num_heads': 4,\n",
    "    'num_layers': 4,\n",
    "    'dropout': 0.1,\n",
    "    'drop_prob_lm': 0.5,\n",
    "    'bos_idx': 0,\n",
    "    'eos_idx': 1,\n",
    "    'pad_idx': 2,\n",
    "    'use_bn': 0,\n",
    "    'beam_size': 3,\n",
    "    'num_workers': 2,\n",
    "    'embeddings_path': embeddings_path,\n",
    "    'reports_json_path': REPORTS_JSON_PATH,\n",
    "    'sample_n': 1,\n",
    "    'group_size': 1,\n",
    "    'lr': 1e-6,\n",
    "    'sample_method': 'beam_search',\n",
    "    'temperature':1.0,\n",
    "    'output_logsoftmax': 1,\n",
    "    'decoding_constraint': 1,\n",
    "    'suppress_UNK': 1,\n",
    "    'block_trigrams': 1,\n",
    "    'length_penalty': 'avg_5'\n",
    "    \n",
    "}\n",
    "debug = False\n",
    "args = argparse.Namespace(**config)\n",
    "split_frac = [0.85, 0.10, 0.05]\n",
    "model = ReportModel(args, tokenizer)\n",
    "\n",
    "trainer = Trainer(args, model, tokenizer, split_frac)\n",
    "# metrics = trainer.train(fast_dev_run=False)\n",
    "# print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f87860e-8a58-49fd-9ae2-14403a227bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(model_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1012b01-6b8a-48e7-84b8-83972bed4eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = model.state_dict()\n",
    "state_dict = {k:v for k,v in ckpt.items() if k in model_dict}\n",
    "model_dict.update(state_dict) \n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f75481d-3d1e-4bc6-811d-fea1fe341e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name  | Type           | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model | ReportGenModel | 19.8 M | train\n",
      "1 | rouge | ROUGEScore     | 0      | train\n",
      "2 | bleu  | BLEUScore      | 0      | train\n",
      "-------------------------------------------------\n",
      "19.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "19.8 M    Total params\n",
      "79.104    Total estimated model params size (MB)\n",
      "244       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a344949d68f849c1a00e8fdef3b4b665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: ['Electrosurgical pneumocyte granulomatous 7% pneumocyte 7% pneumocyte 7% pneumocyte 7% pneumocyte 7% pneumocyte granulomatous 7% Squamous pneumocyte Squamous pneumocyte Squamous 100% Squamous 7% pneumocyte 100% 7% 100% 7% 100% 3 7% Squamous 7% 100% 7% 100% 3 Squamous pneumocyte 100% 7% grade: 7% Squamous 7% Squamous 7% pneumocyte 100% 7%'], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67764cb010aa40b185a7c67939717fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b82a6f0249d419a94e57106d23bc8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: [''], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603b336c36324c0fb4199fc52946b3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: [''], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.133 >= min_delta = 0.0001. New best score: 1.662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1153297f279b4f7e87bc7a36c632c721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: [''], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0001. New best score: 1.654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e2660a361d4faf889a3be4d274093e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: [''], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.0001. New best score: 1.644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badb8ac47eb54d5fa2433cd095fc1dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: ['biopsy;'], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.643\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ec951385d44aaeb6f8c2574d15e24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: ['biopsy;'], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.643\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d8c0a1213b4c32aa12effba3b0ff66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: ['biopsy;'], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869310f8c454451b85531f8540cca3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: ['biopsy;'], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f273b3034e4afab512c579b674f141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, moderately differentiated']\n",
      "pred_texts: [''], target_texts: ['Colon, colonoscopic biopsy; Chronic nonspecific inflammation']\n",
      "pred_texts: [''], target_texts: ['Stomach, endoscopic biopsy; Adenocarcinoma, well differentiated']\n",
      "pred_texts: [''], target_texts: ['Prostate, biopsy; No tumor present']\n",
      "pred_texts: [''], target_texts: ['Breast, biopsy; Invasive carcinoma of no special type, grade III (Tubule formation: 3, Nuclear grade: 3, Mitoses: 2)']\n",
      "pred_texts: [''], target_texts: [\"Prostate, biopsy; Acinar adenocarcinoma, Gleason's score 6 (3+3), grade group 1, tumor volume: 10%\"]\n",
      "pred_texts: ['biopsy;'], target_texts: ['Breast, core-needle biopsy; Papillary neoplasm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 1.643. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(args, model, tokenizer, split_frac)\n",
    "metrics = trainer.train(fast_dev_run=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52c180-de4d-453a-835e-f65d40566597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756b8a6-4fe7-45f4-b52e-2aaacea471e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e71cef-8fc7-4b53-8629-32344eadc23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee1540-81ec-43d3-a1c0-c771c19cb958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
